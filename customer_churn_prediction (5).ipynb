{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/setigogoli/ML-project/blob/main/customer_churn_prediction%20(5).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stainless-prefix",
      "metadata": {
        "id": "stainless-prefix"
      },
      "source": [
        "**<center> <span style=\"color:#0F52BA;font-family:serif; font-size:34px;\">\n",
        "ML Project\\\n",
        "Setayesh Heydari 40104073\\\n",
        "Amir Hossein Shahrabi 401104208\\\n",
        "Amir Abbas Donyadideh 401104113\n",
        "</span> </center>**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "patient-statistics",
      "metadata": {
        "id": "patient-statistics"
      },
      "source": [
        "# Loading libraries and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spiritual-candy",
      "metadata": {
        "id": "spiritual-candy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import missingno as msno\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "failing-consolidation",
      "metadata": {
        "id": "failing-consolidation"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"blastchar/telco-customer-churn\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\", df.head())"
      ],
      "metadata": {
        "id": "QmD9OQl-DIBd"
      },
      "id": "QmD9OQl-DIBd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 1**"
      ],
      "metadata": {
        "id": "tfggWuSSIqSN"
      },
      "id": "tfggWuSSIqSN"
    },
    {
      "cell_type": "markdown",
      "id": "built-collins",
      "metadata": {
        "id": "built-collins"
      },
      "source": [
        "# Understanding the Data(Data Shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fleet-literacy",
      "metadata": {
        "id": "fleet-literacy"
      },
      "source": [
        "Each row represents a customer, each column contains customer’s attributes described on the column Metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "literary-proof",
      "metadata": {
        "id": "literary-proof"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "waiting-pharmaceutical",
      "metadata": {
        "id": "waiting-pharmaceutical"
      },
      "source": [
        "**The data set includes information about:**\n",
        "* **Customers who left within the last month** – the column is called Churn\n",
        "\n",
        "* **Services that each customer has signed up for** – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n",
        "\n",
        "* **Customer account information** - how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n",
        "\n",
        "* **Demographic info about customers** – gender, age range, and if they have partners and dependents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "macro-replication",
      "metadata": {
        "id": "macro-replication"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "complicated-norman",
      "metadata": {
        "id": "complicated-norman"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "delayed-sailing",
      "metadata": {
        "id": "delayed-sailing"
      },
      "outputs": [],
      "source": [
        "df.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coral-recommendation",
      "metadata": {
        "id": "coral-recommendation"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "developed-survivor",
      "metadata": {
        "id": "developed-survivor"
      },
      "source": [
        "\n",
        "* The target the we will use to guide the exploration is **Churn**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oQPiCrgTcHIH",
      "metadata": {
        "id": "oQPiCrgTcHIH"
      },
      "source": [
        "# Find duplicate rows\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LtUzYVgUcFrC",
      "metadata": {
        "id": "LtUzYVgUcFrC"
      },
      "outputs": [],
      "source": [
        "# ===== Find duplicate rows =====\n",
        "\n",
        "duplicate_rows = df.duplicated()\n",
        "\n",
        "print(\"Number of duplicate rows:\", duplicate_rows.sum())\n",
        "\n",
        "# See the duplicate rows (optional)\n",
        "df[duplicate_rows]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VmX5UgmdcVpN",
      "metadata": {
        "id": "VmX5UgmdcVpN"
      },
      "outputs": [],
      "source": [
        "# Remove duplicates (keep first occurrence)\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "print(\"Shape after removing duplicate rows:\", df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "detailed-bradford",
      "metadata": {
        "id": "detailed-bradford"
      },
      "source": [
        "# Find & remove duplicate variables (columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yfc6OwYSc6x1",
      "metadata": {
        "id": "yfc6OwYSc6x1"
      },
      "outputs": [],
      "source": [
        "# ===== Find & remove duplicate variables (columns) =====\n",
        "\n",
        "# 1) Duplicate columns by *name* (exact same column label repeated)\n",
        "dup_name_mask = df.columns.duplicated()\n",
        "dup_name_cols = df.columns[dup_name_mask].tolist()\n",
        "\n",
        "if dup_name_cols:\n",
        "    print(\"Duplicate column names found:\", dup_name_cols)\n",
        "    # Keep first occurrence, drop the rest\n",
        "    df = df.loc[:, ~dup_name_mask]\n",
        "else:\n",
        "    print(\"No duplicate column names found.\")\n",
        "\n",
        "# 2) Duplicate columns by *content* (same values in every row)\n",
        "# Transpose to compare columns as rows, then find duplicates\n",
        "dup_content_mask = df.T.duplicated()\n",
        "dup_content_cols = df.columns[dup_content_mask].tolist()\n",
        "\n",
        "if dup_content_cols:\n",
        "    print(\"Duplicate columns by content found:\", dup_content_cols)\n",
        "    df = df.drop(columns=dup_content_cols)\n",
        "else:\n",
        "    print(\"No duplicate columns by content found.\")\n",
        "\n",
        "print(\"Shape after removing duplicates:\", df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IF7sCq6xfF7-",
      "metadata": {
        "id": "IF7sCq6xfF7-"
      },
      "source": [
        "# Histogram using Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V_qQE-mZfUPP",
      "metadata": {
        "id": "V_qQE-mZfUPP"
      },
      "outputs": [],
      "source": [
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "for i, col in enumerate(numeric_cols, 1):\n",
        "    plt.subplot(2, 2, i)\n",
        "    sns.histplot(df[col], bins=30, kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67HpJNi5guTw",
      "metadata": {
        "id": "67HpJNi5guTw"
      },
      "source": [
        "#Histogram using seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hy9eCDPVfZcP",
      "metadata": {
        "id": "Hy9eCDPVfZcP"
      },
      "outputs": [],
      "source": [
        "# Select numeric columns automatically\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Plot histograms\n",
        "df[numeric_cols].hist(figsize=(12, 8), bins=30)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "re7Kk61ag1Ju",
      "metadata": {
        "id": "re7Kk61ag1Ju"
      },
      "source": [
        "# histogeram other way\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cwwXk6DVfdRB",
      "metadata": {
        "id": "cwwXk6DVfdRB"
      },
      "outputs": [],
      "source": [
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "for col in numeric_cols:\n",
        "    fig = px.histogram(df, x=col, nbins=30, title=f'Distribution of {col}')\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ULKTrASnf43k",
      "metadata": {
        "id": "ULKTrASnf43k"
      },
      "source": [
        "# bar plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDLZ6HXKf2-P",
      "metadata": {
        "id": "hDLZ6HXKf2-P"
      },
      "outputs": [],
      "source": [
        "# Select categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Remove customerID (not useful for plotting)\n",
        "categorical_cols = categorical_cols.drop('customerID')\n",
        "\n",
        "categorical_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "guw_BHGJg907",
      "metadata": {
        "id": "guw_BHGJg907"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 25))\n",
        "\n",
        "for i, col in enumerate(categorical_cols, 1):\n",
        "    plt.subplot(len(categorical_cols)//2 + 1, 2, i)\n",
        "    sns.countplot(x=col, data=df)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a690127f"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the models and their predictions\n",
        "models = {\n",
        "    \"Logistic Regression\": lr_model.predict(X_test),\n",
        "    \"KNN\": knn_model.predict(X_test),\n",
        "    \"Random Forest\": model_rf.predict(X_test),\n",
        "    \"Gradient Boosting\": gb.predict(X_test)\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, (name, y_pred) in enumerate(models.items()):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Churn', 'Churn'])\n",
        "    disp.plot(cmap='Blues', ax=axes[i], colorbar=False)\n",
        "    axes[i].set_title(f'{name} Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    axes[i].set_xlabel('Predicted Label', fontsize=12)\n",
        "    axes[i].set_ylabel('True Label', fontsize=12)\n",
        "\n",
        "plt.suptitle('Comparison of Confusion Matrices', fontsize=18, fontweight='bold', y=1.02)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to prevent suptitle overlap\n",
        "plt.show()"
      ],
      "id": "a690127f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f92ca949"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the models and their predictions\n",
        "models = {\n",
        "    \"Logistic Regression\": lr_model.predict(X_test),\n",
        "    \"KNN\": knn_model.predict(X_test),\n",
        "    \"Random Forest\": model_rf.predict(X_test),\n",
        "    \"Gradient Boosting\": gb.predict(X_test)\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, (name, y_pred) in enumerate(models.items()):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Churn', 'Churn'])\n",
        "    disp.plot(cmap='Blues', ax=axes[i], colorbar=False)\n",
        "    axes[i].set_title(f'{name} Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    axes[i].set_xlabel('Predicted Label', fontsize=12)\n",
        "    axes[i].set_ylabel('True Label', fontsize=12)\n",
        "\n",
        "plt.suptitle('Comparison of Confusion Matrices', fontsize=18, fontweight='bold', y=1.02)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to prevent suptitle overlap\n",
        "plt.show()"
      ],
      "id": "f92ca949",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bar plot other way"
      ],
      "metadata": {
        "id": "IdSNzircgR8a"
      },
      "id": "IdSNzircgR8a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m3RfZ9xFhIRE",
      "metadata": {
        "id": "m3RfZ9xFhIRE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 25))\n",
        "\n",
        "for i, col in enumerate(categorical_cols, 1):\n",
        "    plt.subplot(len(categorical_cols)//2 + 1, 2, i)\n",
        "\n",
        "    df[col].value_counts().plot(kind='bar')\n",
        "\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qypm5AVeheF0",
      "metadata": {
        "id": "Qypm5AVeheF0"
      },
      "outputs": [],
      "source": [
        "for col in categorical_cols:\n",
        "    fig = px.histogram(df, x=col, title=f'Distribution of {col}')\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "maritime-episode",
      "metadata": {
        "id": "maritime-episode"
      },
      "source": [
        "<a id = \"7\" ></a>\n",
        "# <span style=\"font-family:serif; font-size:28px;\"> 4. Visualize missing values </span>\n",
        "<a id = \"missingvalue\" ></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "insured-israeli",
      "metadata": {
        "id": "insured-israeli"
      },
      "outputs": [],
      "source": [
        "# Visualize missing values as a matrix\n",
        "msno.matrix(df);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tropical-humanity",
      "metadata": {
        "id": "tropical-humanity"
      },
      "source": [
        "> Using this matrix we can very quickly find the pattern of missingness in the dataset.\n",
        "* From the above visualisation we can observe that it has no peculiar pattern that stands out. In fact there is no missing data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "enormous-reflection",
      "metadata": {
        "id": "enormous-reflection"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shared-overview",
      "metadata": {
        "id": "shared-overview"
      },
      "source": [
        "# Data Manipulation(finding missing values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "transsexual-prisoner",
      "metadata": {
        "id": "transsexual-prisoner"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['customerID'], axis = 1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "leading-mississippi",
      "metadata": {
        "id": "leading-mississippi"
      },
      "source": [
        "* On deep analysis, we can find some indirect missingness in our data (which can be in form of blankspaces). Let's see that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "asian-consideration",
      "metadata": {
        "id": "asian-consideration"
      },
      "outputs": [],
      "source": [
        "df['TotalCharges'] = pd.to_numeric(df.TotalCharges, errors='coerce')\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "secure-concert",
      "metadata": {
        "id": "secure-concert"
      },
      "source": [
        "* Here we see that the TotalCharges has 11 missing values. Let's check this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compressed-diving",
      "metadata": {
        "id": "compressed-diving"
      },
      "outputs": [],
      "source": [
        "df[np.isnan(df['TotalCharges'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "advanced-ability",
      "metadata": {
        "id": "advanced-ability"
      },
      "source": [
        "* It can also be noted that the Tenure column is 0 for these entries even though the MonthlyCharges column is not empty.\n",
        "\n",
        "Let's see if there are any other 0 values in the tenure column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shaped-maximum",
      "metadata": {
        "id": "shaped-maximum"
      },
      "outputs": [],
      "source": [
        "df[df['tenure'] == 0].index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "young-shade",
      "metadata": {
        "id": "young-shade"
      },
      "source": [
        "* There are no additional missing values in the Tenure column.\n",
        "\n",
        "Let's delete the rows with missing values in Tenure columns since there are only 11 rows and deleting them will not affect the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "surprised-powder",
      "metadata": {
        "id": "surprised-powder"
      },
      "outputs": [],
      "source": [
        "df.drop(labels=df[df['tenure'] == 0].index, axis=0, inplace=True)\n",
        "df[df['tenure'] == 0].index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "outer-works",
      "metadata": {
        "id": "outer-works"
      },
      "source": [
        "> To solve the problem of missing values in TotalCharges column, I decided to fill it with the mean of TotalCharges values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0PDgmJXpO05",
      "metadata": {
        "id": "d0PDgmJXpO05"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "boolean-budapest",
      "metadata": {
        "id": "boolean-budapest"
      },
      "outputs": [],
      "source": [
        "df.fillna(df[\"TotalCharges\"].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "solved-shipping",
      "metadata": {
        "id": "solved-shipping"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "legal-vatican",
      "metadata": {
        "id": "legal-vatican"
      },
      "outputs": [],
      "source": [
        "df[\"SeniorCitizen\"]= df[\"SeniorCitizen\"].map({0: \"No\", 1: \"Yes\"})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pointed-maintenance",
      "metadata": {
        "id": "pointed-maintenance"
      },
      "outputs": [],
      "source": [
        "df[\"InternetService\"].describe(include=['object', 'bool'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wooden-sodium",
      "metadata": {
        "id": "wooden-sodium"
      },
      "outputs": [],
      "source": [
        "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "df[numerical_cols].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bc3-3pCDppY7",
      "metadata": {
        "id": "Bc3-3pCDppY7"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "orange-juvenile",
      "metadata": {
        "id": "orange-juvenile"
      },
      "source": [
        "#distribution analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "returning-assistant",
      "metadata": {
        "id": "returning-assistant"
      },
      "outputs": [],
      "source": [
        "g_labels = ['Male', 'Female']\n",
        "c_labels = ['No', 'Yes']\n",
        "# Create subplots: use 'domain' type for Pie subplot\n",
        "fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\n",
        "fig.add_trace(go.Pie(labels=g_labels, values=df['gender'].value_counts(), name=\"Gender\"),\n",
        "              1, 1)\n",
        "fig.add_trace(go.Pie(labels=c_labels, values=df['Churn'].value_counts(), name=\"Churn\"),\n",
        "              1, 2)\n",
        "\n",
        "# Use `hole` to create a donut-like pie chart\n",
        "fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", textfont_size=16)\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"Gender and Churn Distributions\",\n",
        "    # Add annotations in the center of the donut pies.\n",
        "    annotations=[dict(text='Gender', x=0.16, y=0.5, font_size=20, showarrow=False),\n",
        "                 dict(text='Churn', x=0.84, y=0.5, font_size=20, showarrow=False)])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "commercial-murray",
      "metadata": {
        "id": "commercial-murray"
      },
      "source": [
        "* 26.6 % of customers switched to another firm.\n",
        "* Customers are 49.5 % female and 50.5 % male."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "senior-mistress",
      "metadata": {
        "id": "senior-mistress"
      },
      "outputs": [],
      "source": [
        "df[\"Churn\"][df[\"Churn\"]==\"No\"].groupby(by=df[\"gender\"]).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "residential-rider",
      "metadata": {
        "id": "residential-rider"
      },
      "outputs": [],
      "source": [
        "df[\"Churn\"][df[\"Churn\"]==\"Yes\"].groupby(by=df[\"gender\"]).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "final-embassy",
      "metadata": {
        "id": "final-embassy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "labels =[\"Churn: Yes\",\"Churn:No\"]\n",
        "values = [1869,5163]\n",
        "labels_gender = [\"F\",\"M\",\"F\",\"M\"]\n",
        "sizes_gender = [939,930 , 2544,2619]\n",
        "colors = ['#ff6666', '#66b3ff']\n",
        "colors_gender = ['#c2c2f0','#ffb3e6', '#c2c2f0','#ffb3e6']\n",
        "explode = (0.3,0.3)\n",
        "explode_gender = (0.1,0.1,0.1,0.1)\n",
        "textprops = {\"fontsize\":15}\n",
        "#Plot\n",
        "plt.pie(values, labels=labels,autopct='%1.1f%%',pctdistance=1.08, labeldistance=0.8,colors=colors, startangle=90,frame=True, explode=explode,radius=10, textprops =textprops, counterclock = True, )\n",
        "plt.pie(sizes_gender,labels=labels_gender,colors=colors_gender,startangle=90, explode=explode_gender,radius=7, textprops =textprops, counterclock = True, )\n",
        "#Draw circle\n",
        "centre_circle = plt.Circle((0,0),5,color='black', fc='white',linewidth=0)\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(centre_circle)\n",
        "\n",
        "plt.title('Churn Distribution w.r.t Gender: Male(M), Female(F)', fontsize=15, y=1.1)\n",
        "\n",
        "# show plot\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "interracial-chosen",
      "metadata": {
        "id": "interracial-chosen"
      },
      "source": [
        "* There is negligible difference in customer percentage/ count who chnaged the service provider. Both genders behaved in similar fashion when it comes to migrating to another service provider/firm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "passive-copper",
      "metadata": {
        "id": "passive-copper"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(df, x=\"Churn\", color=\"Contract\", barmode=\"group\", title=\"<b>Customer contract distribution<b>\")\n",
        "fig.update_layout(width=700, height=500, bargap=0.1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seeing-timber",
      "metadata": {
        "id": "seeing-timber"
      },
      "source": [
        "* About 75% of customer with Month-to-Month Contract opted to move out as compared to 13% of customrs with One Year Contract and 3% with Two Year Contract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "governing-makeup",
      "metadata": {
        "id": "governing-makeup"
      },
      "outputs": [],
      "source": [
        "labels = df['PaymentMethod'].unique()\n",
        "values = df['PaymentMethod'].value_counts()\n",
        "\n",
        "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\n",
        "fig.update_layout(title_text=\"<b>Payment Method Distribution</b>\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "enormous-silly",
      "metadata": {
        "id": "enormous-silly"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(df, x=\"Churn\", color=\"PaymentMethod\", title=\"<b>Customer Payment Method distribution w.r.t. Churn</b>\")\n",
        "fig.update_layout(width=700, height=500, bargap=0.1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prospective-trainer",
      "metadata": {
        "id": "prospective-trainer"
      },
      "source": [
        "* Major customers who moved out were having Electronic Check as Payment Method.\n",
        "* Customers who opted for Credit-Card automatic transfer or Bank Automatic Transfer and Mailed Check as Payment Method were less likely to move out.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "charming-hollywood",
      "metadata": {
        "id": "charming-hollywood"
      },
      "outputs": [],
      "source": [
        "df[\"InternetService\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "advanced-destination",
      "metadata": {
        "id": "advanced-destination"
      },
      "outputs": [],
      "source": [
        "df[df[\"gender\"]==\"Male\"][[\"InternetService\", \"Churn\"]].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "useful-plane",
      "metadata": {
        "id": "useful-plane"
      },
      "outputs": [],
      "source": [
        "df[df[\"gender\"]==\"Female\"][[\"InternetService\", \"Churn\"]].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dutch-confusion",
      "metadata": {
        "id": "dutch-confusion"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "  x = [['Churn:No', 'Churn:No', 'Churn:Yes', 'Churn:Yes'],\n",
        "       [\"Female\", \"Male\", \"Female\", \"Male\"]],\n",
        "  y = [965, 992, 219, 240],\n",
        "  name = 'DSL',\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "  x = [['Churn:No', 'Churn:No', 'Churn:Yes', 'Churn:Yes'],\n",
        "       [\"Female\", \"Male\", \"Female\", \"Male\"]],\n",
        "  y = [889, 910, 664, 633],\n",
        "  name = 'Fiber optic',\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "  x = [['Churn:No', 'Churn:No', 'Churn:Yes', 'Churn:Yes'],\n",
        "       [\"Female\", \"Male\", \"Female\", \"Male\"]],\n",
        "  y = [690, 717, 56, 57],\n",
        "  name = 'No Internet',\n",
        "))\n",
        "\n",
        "fig.update_layout(title_text=\"<b>Churn Distribution w.r.t. Internet Service and Gender</b>\")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "according-phone",
      "metadata": {
        "id": "according-phone"
      },
      "source": [
        "* A lot of customers choose the Fiber optic service and it's also evident that the customers who use Fiber optic have high churn rate, this might suggest a dissatisfaction with this type of internet service.\n",
        "* Customers having DSL service are majority in number and have less churn rate compared to Fibre optic service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "descending-hours",
      "metadata": {
        "id": "descending-hours"
      },
      "outputs": [],
      "source": [
        "color_map = {\"Yes\": \"#FF97FF\", \"No\": \"#AB63FA\"}\n",
        "fig = px.histogram(df, x=\"Churn\", color=\"Dependents\", barmode=\"group\", title=\"<b>Dependents distribution</b>\", color_discrete_map=color_map)\n",
        "fig.update_layout(width=700, height=500, bargap=0.1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "protective-president",
      "metadata": {
        "id": "protective-president"
      },
      "source": [
        "* Customers without dependents are more likely to churn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metropolitan-carpet",
      "metadata": {
        "id": "metropolitan-carpet"
      },
      "outputs": [],
      "source": [
        "color_map = {\"Yes\": '#FFA15A', \"No\": '#00CC96'}\n",
        "fig = px.histogram(df, x=\"Churn\", color=\"Partner\", barmode=\"group\", title=\"<b>Chrun distribution w.r.t. Partners</b>\", color_discrete_map=color_map)\n",
        "fig.update_layout(width=700, height=500, bargap=0.1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fleet-perry",
      "metadata": {
        "id": "fleet-perry"
      },
      "source": [
        "* Customers that doesn't have partners are more likely to churn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "silent-institution",
      "metadata": {
        "id": "silent-institution"
      },
      "outputs": [],
      "source": [
        "color_map = {\"Yes\": '#00CC96', \"No\": '#B6E880'}\n",
        "fig = px.histogram(df, x=\"Churn\", color=\"SeniorCitizen\", title=\"<b>Chrun distribution w.r.t. Senior Citizen</b>\", color_discrete_map=color_map)\n",
        "fig.update_layout(width=700, height=500, bargap=0.1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tamil-pathology",
      "metadata": {
        "id": "tamil-pathology"
      },
      "source": [
        "* It can be observed that the fraction of senior citizen is very less.\n",
        "* Most of the senior citizens churn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "assisted-capability",
      "metadata": {
        "id": "assisted-capability"
      },
      "outputs": [],
      "source": [
        "color_map = {\"Yes\": \"#FF97FF\", \"No\": \"#AB63FA\"}\n",
        "fig = px.histogram(df, x=\"Churn\", color=\"OnlineSecurity\", barmode=\"group\", title=\"<b>Churn w.r.t Online Security</b>\", color_discrete_map=color_map)\n",
        "fig.update_layout(width=700, height=500, bargap=0.1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "quick-blank",
      "metadata": {
        "id": "quick-blank"
      },
      "source": [
        "* Most customers churn in the absence of online security,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adverse-ultimate",
      "metadata": {
        "id": "adverse-ultimate"
      },
      "outputs": [],
      "source": [
        "color_map = {\"Yes\": '#FFA15A', \"No\": '#00CC96'}\n",
        "fig = px.histogram(df, x=\"Churn\", color=\"PaperlessBilling\",  title=\"<b>Chrun distribution w.r.t. Paperless Billing</b>\", color_discrete_map=color_map)\n",
        "fig.update_layout(width=700, height=500, bargap=0.1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "social-accordance",
      "metadata": {
        "id": "social-accordance"
      },
      "source": [
        "* Customers with Paperless Billing are most likely to churn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "medieval-exclusion",
      "metadata": {
        "id": "medieval-exclusion"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(df, x=\"Churn\", color=\"TechSupport\",barmode=\"group\",  title=\"<b>Chrun distribution w.r.t. TechSupport</b>\")\n",
        "fig.update_layout(width=700, height=500, bargap=0.1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "injured-weekend",
      "metadata": {
        "id": "injured-weekend"
      },
      "source": [
        "* Customers with no TechSupport are most likely to migrate to another service provider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ordinary-token",
      "metadata": {
        "id": "ordinary-token"
      },
      "outputs": [],
      "source": [
        "color_map = {\"Yes\": '#00CC96', \"No\": '#B6E880'}\n",
        "fig = px.histogram(df, x=\"Churn\", color=\"PhoneService\", title=\"<b>Chrun distribution w.r.t. Phone Service</b>\", color_discrete_map=color_map)\n",
        "fig.update_layout(width=700, height=500, bargap=0.1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fleet-reasoning",
      "metadata": {
        "id": "fleet-reasoning"
      },
      "source": [
        "* Very small fraction of customers don't have a phone service and out of that, 1/3rd Customers are more likely to churn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "relevant-henry",
      "metadata": {
        "id": "relevant-henry"
      },
      "outputs": [],
      "source": [
        "sns.set_context(\"paper\",font_scale=1.1)\n",
        "ax = sns.kdeplot(df.MonthlyCharges[(df[\"Churn\"] == 'No') ],\n",
        "                color=\"Red\", shade = True);\n",
        "ax = sns.kdeplot(df.MonthlyCharges[(df[\"Churn\"] == 'Yes') ],\n",
        "                ax =ax, color=\"Blue\", shade= True);\n",
        "ax.legend([\"Not Churn\",\"Churn\"],loc='upper right');\n",
        "ax.set_ylabel('Density');\n",
        "ax.set_xlabel('Monthly Charges');\n",
        "ax.set_title('Distribution of monthly charges by churn');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "engaging-spice",
      "metadata": {
        "id": "engaging-spice"
      },
      "source": [
        "* Customers with higher Monthly Charges are also more likely to churn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deluxe-basics",
      "metadata": {
        "id": "deluxe-basics"
      },
      "outputs": [],
      "source": [
        "ax = sns.kdeplot(df.TotalCharges[(df[\"Churn\"] == 'No') ],\n",
        "                color=\"Gold\", shade = True);\n",
        "ax = sns.kdeplot(df.TotalCharges[(df[\"Churn\"] == 'Yes') ],\n",
        "                ax =ax, color=\"Green\", shade= True);\n",
        "ax.legend([\"Not Chu0rn\",\"Churn\"],loc='upper right');\n",
        "ax.set_ylabel('Density');\n",
        "ax.set_xlabel('Total Charges');\n",
        "ax.set_title('Distribution of total charges by churn');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Heat Map"
      ],
      "metadata": {
        "id": "CUYpp1CdkUXC"
      },
      "id": "CUYpp1CdkUXC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "intermediate-olive",
      "metadata": {
        "id": "intermediate-olive"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(25, 10))\n",
        "\n",
        "corr = df.apply(lambda x: pd.factorize(x)[0]).corr()\n",
        "\n",
        "# Create a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "ax = sns.heatmap(corr, mask=mask, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, linewidths=.2, cmap='coolwarm', vmin=-1, vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "social-freight",
      "metadata": {
        "id": "social-freight"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KhpTpmR3mtT2",
      "metadata": {
        "id": "KhpTpmR3mtT2"
      },
      "source": [
        "# Box Plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bOg9MvVmjBa",
      "metadata": {
        "id": "5bOg9MvVmjBa"
      },
      "source": [
        "Tenure vs Churn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8xDqY1A9mZMA",
      "metadata": {
        "id": "8xDqY1A9mZMA"
      },
      "outputs": [],
      "source": [
        "fig = px.box(df, x='Churn', y='tenure', color='Churn')\n",
        "\n",
        "fig.update_yaxes(title_text='Tenure (Months)')\n",
        "fig.update_xaxes(title_text='Churn')\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize=True,\n",
        "    width=750,\n",
        "    height=600,\n",
        "    title_font=dict(size=25, family='Courier'),\n",
        "    title='<b>Tenure vs Churn</b>'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mFcNVPn8mQiu",
      "metadata": {
        "id": "mFcNVPn8mQiu"
      },
      "source": [
        "MonthlyCharges vs Churn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZlFmGBKomQPI",
      "metadata": {
        "id": "ZlFmGBKomQPI"
      },
      "outputs": [],
      "source": [
        "fig = px.box(df, x='Churn', y='MonthlyCharges', color='Churn')\n",
        "\n",
        "fig.update_yaxes(title_text='Monthly Charges')\n",
        "fig.update_xaxes(title_text='Churn')\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize=True,\n",
        "    width=750,\n",
        "    height=600,\n",
        "    title_font=dict(size=25, family='Courier'),\n",
        "    title='<b>Monthly Charges vs Churn</b>'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QfPD28cnm1vc",
      "metadata": {
        "id": "QfPD28cnm1vc"
      },
      "source": [
        "TotalCharges vs Churn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vG7YuExOm4o9",
      "metadata": {
        "id": "vG7YuExOm4o9"
      },
      "outputs": [],
      "source": [
        "#Make sure it's numeric first\n",
        "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"TotalCharges\"])\n",
        "\n",
        "fig = px.box(df, x='Churn', y='TotalCharges', color='Churn')\n",
        "\n",
        "fig.update_yaxes(title_text='Total Charges')\n",
        "fig.update_xaxes(title_text='Churn')\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize=True,\n",
        "    width=750,\n",
        "    height=600,\n",
        "    title_font=dict(size=25, family='Courier'),\n",
        "    title='<b>Total Charges vs Churn</b>'\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LEF-bI5NnKAA",
      "metadata": {
        "id": "LEF-bI5NnKAA"
      },
      "source": [
        "all in one code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3yfoiKm4nEbU",
      "metadata": {
        "id": "3yfoiKm4nEbU"
      },
      "outputs": [],
      "source": [
        "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "for col in numeric_cols:\n",
        "    fig = px.box(df, x='Churn', y=col, color='Churn',\n",
        "                 title=f'<b>{col} vs Churn</b>')\n",
        "\n",
        "    fig.update_layout(width=750, height=600)\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "premium-prisoner",
      "metadata": {
        "id": "premium-prisoner"
      },
      "source": [
        "* New customers are more likely to churn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 2**"
      ],
      "metadata": {
        "id": "AThSlamuH3P0"
      },
      "id": "AThSlamuH3P0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implement Imputation\n"
      ],
      "metadata": {
        "id": "43zJNrQ7i-iu"
      },
      "id": "43zJNrQ7i-iu"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert TotalCharges to numeric (blank strings -> NaN)\n",
        "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "\n",
        "# Separate numeric and categorical columns\n",
        "num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns\n"
      ],
      "metadata": {
        "id": "AOyy_oiwjGvB"
      },
      "id": "AOyy_oiwjGvB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numeric: fill NaN with median (robust)\n",
        "for col in num_cols:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# Categorical: fill NaN with most frequent value (mode)\n",
        "for col in cat_cols:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "# Check\n",
        "print(\"Missing values after imputation:\\n\", df.isnull().sum()[df.isnull().sum() > 0])\n"
      ],
      "metadata": {
        "id": "p6d9m8LCjHh7"
      },
      "id": "p6d9m8LCjHh7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "another way"
      ],
      "metadata": {
        "id": "suhxhBeUjOB9"
      },
      "id": "suhxhBeUjOB9"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Convert TotalCharges to numeric\n",
        "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "\n",
        "# Split X/y\n",
        "X = df.drop(columns=[\"Churn\"])\n",
        "y = df[\"Churn\"]\n",
        "\n",
        "# Identify columns\n",
        "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "cat_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns\n",
        "\n",
        "# Preprocessors\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, num_cols),\n",
        "        (\"cat\", categorical_transformer, cat_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Example split (optional)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Fit/transform\n",
        "X_train_prepared = preprocessor.fit_transform(X_train)\n",
        "X_test_prepared  = preprocessor.transform(X_test)\n",
        "\n",
        "print(\"Done. Shapes:\", X_train_prepared.shape, X_test_prepared.shape)\n"
      ],
      "metadata": {
        "id": "3Kn9CJ7cjPk9"
      },
      "id": "3Kn9CJ7cjPk9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lable Encoding"
      ],
      "metadata": {
        "id": "5q0VFGKujUSQ"
      },
      "id": "5q0VFGKujUSQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Select categorical columns\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Find binary columns (exactly 2 unique values)\n",
        "binary_cols = [col for col in cat_cols if df[col].nunique() == 2]\n",
        "\n",
        "print(\"Binary columns:\")\n",
        "print(binary_cols)\n"
      ],
      "metadata": {
        "id": "wWRjLXHsjZaU"
      },
      "id": "wWRjLXHsjZaU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "for col in binary_cols:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "IjCqTZccjb6R"
      },
      "id": "IjCqTZccjb6R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[binary_cols].head()"
      ],
      "metadata": {
        "id": "hJ7EHB8yja1B"
      },
      "id": "hJ7EHB8yja1B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#One Hot Encoding"
      ],
      "metadata": {
        "id": "u7aAzHRejjhf"
      },
      "id": "u7aAzHRejjhf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Select categorical columns\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Select columns with more than 2 unique values\n",
        "multi_cols = [col for col in cat_cols if df[col].nunique() > 2]\n",
        "\n",
        "print(\"Multi-category columns:\")\n",
        "print(multi_cols)\n"
      ],
      "metadata": {
        "id": "3YX-iSzDjowc"
      },
      "id": "3YX-iSzDjowc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, columns=multi_cols, drop_first=True)\n",
        "\n",
        "df.head()\n",
        "print(\"New shape after One-Hot Encoding:\", df.shape)\n"
      ],
      "metadata": {
        "id": "DFoWnvxnjrVH"
      },
      "id": "DFoWnvxnjrVH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "another way"
      ],
      "metadata": {
        "id": "pE-LW053j0Hw"
      },
      "id": "pE-LW053j0Hw"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# 1) Recompute multi-category columns from CURRENT df (only object columns with >2 unique values)\n",
        "multi_cols = [c for c in df.columns if df[c].dtype == \"object\" and df[c].nunique() > 2]\n",
        "\n",
        "print(\"Multi-category columns to one-hot encode:\", multi_cols)\n",
        "\n",
        "# If nothing left to encode, stop safely\n",
        "if len(multi_cols) == 0:\n",
        "    print(\"No multi-category object columns left to encode.\")\n",
        "else:\n",
        "    # 2) One-hot encode\n",
        "    ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n",
        "    encoded = ohe.fit_transform(df[multi_cols])\n",
        "\n",
        "    encoded_df = pd.DataFrame(\n",
        "        encoded,\n",
        "        columns=ohe.get_feature_names_out(multi_cols),\n",
        "        index=df.index\n",
        "    )\n",
        "\n",
        "    # 3) Replace original columns with encoded columns\n",
        "    df = df.drop(columns=multi_cols)\n",
        "    df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "    print(\"Done. New shape:\", df.shape)\n"
      ],
      "metadata": {
        "id": "9sUXz6Hhj4Ux"
      },
      "id": "9sUXz6Hhj4Ux",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_cols = [c for c in df.columns if df[c].dtype == \"object\" and df[c].nunique() >= 2]\n"
      ],
      "metadata": {
        "id": "ULKGpHm-kCxq"
      },
      "id": "ULKGpHm-kCxq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "basic-dependence",
      "metadata": {
        "id": "basic-dependence"
      },
      "source": [
        "## train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wicked-directive",
      "metadata": {
        "id": "wicked-directive"
      },
      "outputs": [],
      "source": [
        "def object_to_int(dataframe_series):\n",
        "    if dataframe_series.dtype=='object':\n",
        "        dataframe_series = LabelEncoder().fit_transform(dataframe_series)\n",
        "    return dataframe_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "verbal-culture",
      "metadata": {
        "id": "verbal-culture"
      },
      "outputs": [],
      "source": [
        "df = df.apply(lambda x: object_to_int(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cheap-writing",
      "metadata": {
        "id": "cheap-writing"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,7))\n",
        "df.corr()['Churn'].sort_values(ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "confirmed-daisy",
      "metadata": {
        "id": "confirmed-daisy"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns = ['Churn'])\n",
        "y = df['Churn'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "medium-architecture",
      "metadata": {
        "id": "medium-architecture"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, random_state = 40, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "moderate-civilian",
      "metadata": {
        "id": "moderate-civilian"
      },
      "outputs": [],
      "source": [
        "def distplot(feature, frame, color='r'):\n",
        "    plt.figure(figsize=(8,3))\n",
        "    plt.title(\"Distribution for {}\".format(feature))\n",
        "    ax = sns.distplot(frame[feature], color= color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "understanding-suite",
      "metadata": {
        "id": "understanding-suite"
      },
      "outputs": [],
      "source": [
        "num_cols = [\"tenure\", 'MonthlyCharges', 'TotalCharges']\n",
        "for feat in num_cols: distplot(feat, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sealed-representation",
      "metadata": {
        "id": "sealed-representation"
      },
      "source": [
        "Since the numerical features are distributed over different value ranges, I will use standard scalar to scale them down to the same range."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "still-gabriel",
      "metadata": {
        "id": "still-gabriel"
      },
      "source": [
        "#Standard scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rational-massachusetts",
      "metadata": {
        "id": "rational-massachusetts"
      },
      "outputs": [],
      "source": [
        "df_std = pd.DataFrame(StandardScaler().fit_transform(df[num_cols].astype('float64')),\n",
        "                       columns=num_cols)\n",
        "for feat in numerical_cols: distplot(feat, df_std, color='c')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mysterious-pasta",
      "metadata": {
        "id": "mysterious-pasta"
      },
      "outputs": [],
      "source": [
        "# Divide the columns into 3 categories, one ofor standardisation, one for label encoding and one for one hot encoding\n",
        "\n",
        "cat_cols_ohe =['PaymentMethod', 'Contract', 'InternetService'] # those that need one-hot encoding\n",
        "cat_cols_le = list(set(X_train.columns)- set(num_cols) - set(cat_cols_ohe)) #those that need label encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "medium-balloon",
      "metadata": {
        "id": "medium-balloon"
      },
      "outputs": [],
      "source": [
        "scaler= StandardScaler()\n",
        "\n",
        "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "X_test[num_cols] = scaler.transform(X_test[num_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 3**"
      ],
      "metadata": {
        "id": "GCfPOgoeIvTI"
      },
      "id": "GCfPOgoeIvTI"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ── make sure boolean columns are int (required for chi2) ──\n",
        "bool_cols = df.select_dtypes(include='bool').columns\n",
        "df[bool_cols] = df[bool_cols].astype(int)"
      ],
      "metadata": {
        "id": "52kKQNEuT4s7"
      },
      "id": "52kKQNEuT4s7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering — Creating 2 new meaningful features\n"
      ],
      "metadata": {
        "id": "tRSAdBAEVKza"
      },
      "id": "tRSAdBAEVKza"
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Feature 1: Tenure Group (بازه‌بندی tenure) ──────────────\n",
        "# Binning tenure into meaningful loyalty segments\n",
        "def tenure_group(tenure):\n",
        "    if tenure <= 12:\n",
        "        return 0   # New Customer (0–1 year)\n",
        "    elif tenure <= 24:\n",
        "        return 1   # Developing (1–2 years)\n",
        "    elif tenure <= 48:\n",
        "        return 2   # Established (2–4 years)\n",
        "    else:\n",
        "        return 3   # Loyal (4+ years)\n",
        "\n",
        "df['Tenure_Group'] = df['tenure'].apply(tenure_group)\n",
        "\n",
        "print(\"Tenure Group distribution:\")\n",
        "print(df['Tenure_Group'].value_counts().sort_index())\n",
        "print()\n",
        "\n",
        "# ── Feature 2: Charges per Month Ratio ──────────────────────\n",
        "# TotalCharges / tenure gives average spend per month\n",
        "# This reveals if a customer's spending is consistent or changed\n",
        "df['Avg_Monthly_Spend'] = df['TotalCharges'] / (df['tenure'] + 1)\n",
        "# (+1 to avoid division by zero for tenure=0)\n",
        "\n",
        "# ── Feature 3: Service Count ────────────────────────────────\n",
        "# How many add-on services does each customer subscribe to?\n",
        "# More services → higher switching cost → lower churn probability\n",
        "service_cols = [\n",
        "    'MultipleLines_Yes',\n",
        "    'OnlineSecurity_Yes',\n",
        "    'OnlineBackup_Yes',\n",
        "    'DeviceProtection_Yes',\n",
        "    'TechSupport_Yes',\n",
        "    'StreamingTV_Yes',\n",
        "    'StreamingMovies_Yes'\n",
        "]\n",
        "df['Service_Count'] = df[service_cols].sum(axis=1)\n",
        "\n",
        "print(\"New features added:\")\n",
        "print(df[['tenure', 'Tenure_Group', 'TotalCharges',\n",
        "          'Avg_Monthly_Spend', 'Service_Count']].head(10))\n",
        "print()\n",
        "print(\"Correlation of new features with Churn:\")\n",
        "print(df[['Tenure_Group', 'Avg_Monthly_Spend',\n",
        "          'Service_Count', 'Churn']].corr()['Churn'])\n"
      ],
      "metadata": {
        "id": "WQhv1mcbVL7u"
      },
      "id": "WQhv1mcbVL7u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter-Based Selection (Chi-Squared + ANOVA)"
      ],
      "metadata": {
        "id": "Yd60mF_EVS_j"
      },
      "id": "Yd60mF_EVS_j"
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target\n",
        "X = df.drop(columns=['Churn'])\n",
        "y = df['Churn']\n",
        "\n",
        "# ── Scale numerical features to [0,1] for Chi2 ──────────────\n",
        "# Chi2 requires non-negative values\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X),\n",
        "    columns=X.columns\n",
        ")\n"
      ],
      "metadata": {
        "id": "WujdFkDgVTtL"
      },
      "id": "WujdFkDgVTtL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chi-Squared Test (for categorical/binary features)"
      ],
      "metadata": {
        "id": "FiI4QMS-VYxo"
      },
      "id": "FiI4QMS-VYxo"
    },
    {
      "cell_type": "code",
      "source": [
        "chi2_selector = SelectKBest(score_func=chi2, k='all')\n",
        "chi2_selector.fit(X_scaled, y)\n",
        "\n",
        "chi2_scores = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Chi2_Score': chi2_selector.scores_,\n",
        "    'P_Value': chi2_selector.pvalues_\n",
        "}).sort_values('Chi2_Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"=\" * 55)\n",
        "print(\"Chi-Squared Scores (Top 15):\")\n",
        "print(\"=\" * 55)\n",
        "print(chi2_scores.head(15).to_string(index=False))\n",
        "print()\n",
        "\n",
        "# Plot Chi2\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(\n",
        "    data=chi2_scores.head(15),\n",
        "    x='Chi2_Score',\n",
        "    y='Feature',\n",
        "    palette='Blues_r'\n",
        ")\n",
        "plt.title('Top 15 Features — Chi-Squared Test', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Chi² Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qj6qv3gvVZdX"
      },
      "id": "Qj6qv3gvVZdX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANOVA F-Test (better for continuous numerical features)"
      ],
      "metadata": {
        "id": "Y4ko-DfSVd3n"
      },
      "id": "Y4ko-DfSVd3n"
    },
    {
      "cell_type": "code",
      "source": [
        "anova_selector = SelectKBest(score_func=f_classif, k='all')\n",
        "anova_selector.fit(X_scaled, y)\n",
        "\n",
        "anova_scores = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'ANOVA_F_Score': anova_selector.scores_,\n",
        "    'P_Value': anova_selector.pvalues_\n",
        "}).sort_values('ANOVA_F_Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"=\" * 55)\n",
        "print(\"ANOVA F-Scores (Top 15):\")\n",
        "print(\"=\" * 55)\n",
        "print(anova_scores.head(15).to_string(index=False))\n",
        "print()\n",
        "\n",
        "# Plot ANOVA\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(\n",
        "    data=anova_scores.head(15),\n",
        "    x='ANOVA_F_Score',\n",
        "    y='Feature',\n",
        "    palette='Greens_r'\n",
        ")\n",
        "plt.title('Top 15 Features — ANOVA F-Test', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('ANOVA F-Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GFJz5mEwVen_"
      },
      "id": "GFJz5mEwVen_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combined Filter Score"
      ],
      "metadata": {
        "id": "XC3fGnWhVlMP"
      },
      "id": "XC3fGnWhVlMP"
    },
    {
      "cell_type": "code",
      "source": [
        "filter_combined = chi2_scores[['Feature', 'Chi2_Score']].merge(\n",
        "    anova_scores[['Feature', 'ANOVA_F_Score']], on='Feature'\n",
        ")\n",
        "# Normalize both scores to [0,1] and average them\n",
        "filter_combined['Chi2_norm']  = (filter_combined['Chi2_Score'] /\n",
        "                                  filter_combined['Chi2_Score'].max())\n",
        "filter_combined['ANOVA_norm'] = (filter_combined['ANOVA_F_Score'] /\n",
        "                                  filter_combined['ANOVA_F_Score'].max())\n",
        "filter_combined['Combined_Score'] = (filter_combined['Chi2_norm'] +\n",
        "                                      filter_combined['ANOVA_norm']) / 2\n",
        "filter_combined = filter_combined.sort_values(\n",
        "    'Combined_Score', ascending=False\n",
        ").reset_index(drop=True)\n",
        "\n",
        "top15_filter = filter_combined.head(15)['Feature'].tolist()\n",
        "print(\"Top 15 Features by Combined Filter Score:\")\n",
        "print(top15_filter)"
      ],
      "metadata": {
        "id": "mf0A3D9OVkGY"
      },
      "id": "mf0A3D9OVkGY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso Regression (L1 — drives irrelevant features to 0)"
      ],
      "metadata": {
        "id": "zu1nqXGRVopP"
      },
      "id": "zu1nqXGRVopP"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Redefine X and y using df_model (the feature-selected DataFrame)\n",
        "X = df_model.drop(columns=['Churn'])\n",
        "y = df_model['Churn']\n",
        "\n",
        "# 2. Re-perform train-test split using this X and y\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=40, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Identify numerical columns within the selected features for scaling\n",
        "numerical_cols_for_scaling_selected = ['tenure', 'Avg_Monthly_Spend', 'TotalCharges', 'MonthlyCharges']\n",
        "current_numerical_cols = [col for col in numerical_cols_for_scaling_selected if col in X_train.columns]\n",
        "\n",
        "# 4. Apply StandardScaler to the numerical columns of the newly split X_train and X_test\n",
        "scaler = StandardScaler()\n",
        "if current_numerical_cols:\n",
        "    X_train[current_numerical_cols] = scaler.fit_transform(X_train[current_numerical_cols])\n",
        "    X_test[current_numerical_cols] = scaler.transform(X_test[current_numerical_cols])\n",
        "\n",
        "print(\"X_train and X_test have been updated to use final selected features and scaled.\")\n",
        "print(f\"New X_train shape: {X_train.shape}\")\n",
        "print(f\"New X_test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "AmEgansuVxhH"
      },
      "id": "AmEgansuVxhH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Feature Importance"
      ],
      "metadata": {
        "id": "uHrDUG4MWFFH"
      },
      "id": "uHrDUG4MWFFH"
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "rf_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'RF_Importance': rf.feature_importances_\n",
        "}).sort_values('RF_Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"=\" * 55)\n",
        "print(\"Random Forest — Top 15 Feature Importances:\")\n",
        "print(\"=\" * 55)\n",
        "print(rf_importance.head(15).to_string(index=False))\n",
        "print()\n",
        "\n",
        "# Plot RF\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(\n",
        "    data=rf_importance.head(15),\n",
        "    x='RF_Importance',\n",
        "    y='Feature',\n",
        "    palette='Purples_r'\n",
        ")\n",
        "plt.title('Feature Importance — Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7RxYJnHJWFnb"
      },
      "id": "7RxYJnHJWFnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Feature Subset — Combining all methods"
      ],
      "metadata": {
        "id": "tzhqViMCWKtz"
      },
      "id": "tzhqViMCWKtz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank features across all 3 methods\n",
        "all_features = X.columns.tolist()\n",
        "\n",
        "# Rank by each method (lower rank = more important)\n",
        "filter_rank = {f: i for i, f in enumerate(top15_filter)}\n",
        "lasso_rank  = {f: i for i, f in\n",
        "               enumerate(lasso_importance['Feature'].tolist())}\n",
        "rf_rank     = {f: i for i, f in\n",
        "               enumerate(rf_importance['Feature'].tolist())}\n",
        "\n",
        "ranking_df = pd.DataFrame({'Feature': all_features})\n",
        "ranking_df['Filter_Rank'] = ranking_df['Feature'].map(\n",
        "    lambda f: filter_rank.get(f, len(all_features))\n",
        ")\n",
        "ranking_df['Lasso_Rank']  = ranking_df['Feature'].map(\n",
        "    lambda f: lasso_rank.get(f, len(all_features))\n",
        ")\n",
        "ranking_df['RF_Rank']     = ranking_df['Feature'].map(\n",
        "    lambda f: rf_rank.get(f, len(all_features))\n",
        ")\n",
        "ranking_df['Avg_Rank']    = ranking_df[\n",
        "    ['Filter_Rank', 'Lasso_Rank', 'RF_Rank']\n",
        "].mean(axis=1)\n",
        "\n",
        "ranking_df = ranking_df.sort_values('Avg_Rank').reset_index(drop=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Final Feature Ranking (All Methods Combined):\")\n",
        "print(\"=\" * 60)\n",
        "print(ranking_df.head(15).to_string(index=False))\n",
        "\n",
        "# ── Select Top 12 Final Features ─────────────────────────────\n",
        "final_features = ranking_df.head(12)['Feature'].tolist()\n",
        "print(\"\\n✅ Final Selected Features:\")\n",
        "for i, f in enumerate(final_features, 1):\n",
        "    print(f\"  {i:2}. {f}\")\n",
        "\n",
        "# ── Final dataframe ready for modeling ───────────────────────\n",
        "df_model = df[final_features + ['Churn']].copy()\n",
        "print(f\"\\ndf_model shape: {df_model.shape}\")\n",
        "print(df_model.head())\n"
      ],
      "metadata": {
        "id": "BmevYEvyWL7X"
      },
      "id": "BmevYEvyWL7X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# E. Textual Justification for Final Feature Subset\n",
        "# ============================================================\n",
        "\n",
        "justification = \"\"\"\n",
        "╔══════════════════════════════════════════════════════════════╗\n",
        "║         PHASE 3 — FINAL FEATURE SUBSET JUSTIFICATION        ║\n",
        "╚══════════════════════════════════════════════════════════════╝\n",
        "\n",
        "1. FEATURE ENGINEERING\n",
        "   ─────────────────────────────────────────────────────────\n",
        "   • Tenure_Group: Binning tenure into 4 loyalty segments\n",
        "     (New/Developing/Established/Loyal) captures non-linear\n",
        "     churn behavior — new customers churn at much higher rates.\n",
        "\n",
        "   • Avg_Monthly_Spend: TotalCharges / (tenure+1) captures\n",
        "     whether a customer's spending is rising or falling over\n",
        "     time, which is more informative than raw TotalCharges alone.\n",
        "\n",
        "   • Service_Count: The total number of add-on services acts\n",
        "     as a proxy for switching cost — customers with more\n",
        "     services face higher friction when leaving.\n",
        "\n",
        "2. FILTER-BASED SELECTION (Chi2 + ANOVA)\n",
        "   ─────────────────────────────────────────────────────────\n",
        "   • Chi-Squared identified categorical features most\n",
        "     statistically dependent on Churn (p < 0.05).\n",
        "   • ANOVA F-Test confirmed continuous features (tenure,\n",
        "     MonthlyCharges, TotalCharges) with highest group\n",
        "     mean differences between churned/non-churned customers.\n",
        "   • Features failing both tests (p > 0.05 in both) were\n",
        "     considered statistically insignificant and down-ranked.\n",
        "\n",
        "3. MODEL-BASED SELECTION\n",
        "   ─────────────────────────────────────────────────────────\n",
        "   • Lasso (L1): By penalizing coefficients toward zero,\n",
        "     Lasso automatically eliminated multicollinear and\n",
        "     redundant features. Only features surviving L1\n",
        "     shrinkage carry independent predictive signal.\n",
        "   • Random Forest: Impurity-based importance scores\n",
        "     capture non-linear relationships and interactions\n",
        "     that linear methods like Lasso may miss.\n",
        "\n",
        "4. FINAL SELECTION RATIONALE\n",
        "   ─────────────────────────────────────────────────────────\n",
        "   The final 12 features were chosen by averaging ranks\n",
        "   across all three methods. This ensemble approach is more\n",
        "   robust than relying on any single method:\n",
        "   - It avoids overfitting to one selection criterion.\n",
        "   - Features consistently ranked high across methods\n",
        "     are genuinely predictive, not method-specific artifacts.\n",
        "   - Multicollinear OHE dummy pairs (e.g.,\n",
        "     'InternetService_No' vs 'InternetService_Fiber optic')\n",
        "     were deduplicated keeping only the higher-ranked one.\n",
        "\"\"\"\n",
        "print(justification)\n"
      ],
      "metadata": {
        "id": "40j2eGJIWQ-j"
      },
      "id": "40j2eGJIWQ-j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4"
      ],
      "metadata": {
        "id": "wGUobRGVXAj9"
      },
      "id": "wGUobRGVXAj9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Class Distribution"
      ],
      "metadata": {
        "id": "v9nVe-GXXFaz"
      },
      "id": "v9nVe-GXXFaz"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# ── Current distribution ─────────────────────────────────────\n",
        "print(\"=\" * 55)\n",
        "print(\"Class Distribution in Full Dataset:\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"  Not Churned (0): {Counter(y)[0]:,}  ({Counter(y)[0]/len(y)*100:.1f}%)\")\n",
        "print(f\"  Churned     (1): {Counter(y)[1]:,}  ({Counter(y)[1]/len(y)*100:.1f}%)\")\n",
        "print(f\"  Imbalance Ratio: {Counter(y)[0]/Counter(y)[1]:.2f} : 1\")\n",
        "\n",
        "# ── Visualize ────────────────────────────────────────────────\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Bar chart\n",
        "counts = pd.Series(y).value_counts().sort_index()\n",
        "axes[0].bar(['Not Churned (0)', 'Churned (1)'],\n",
        "            counts.values,\n",
        "            color=['#2196F3', '#F44336'],\n",
        "            edgecolor='black', width=0.5)\n",
        "axes[0].set_title('Class Distribution — Before Balancing',\n",
        "                   fontsize=13, fontweight='bold')\n",
        "axes[0].set_ylabel('Count')\n",
        "for i, v in enumerate(counts.values):\n",
        "    axes[0].text(i, v + 50, f'{v:,}\\n({v/len(y)*100:.1f}%)',\n",
        "                 ha='center', fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(counts.values,\n",
        "            labels=['Not Churned', 'Churned'],\n",
        "            colors=['#2196F3', '#F44336'],\n",
        "            autopct='%1.1f%%',\n",
        "            startangle=90,\n",
        "            explode=(0, 0.05))\n",
        "axes[1].set_title('Churn Rate', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LHNeOoWrXCBz"
      },
      "id": "LHNeOoWrXCBz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE — Synthetic Minority Over-Sampling Technique"
      ],
      "metadata": {
        "id": "gOCwNmIUXRtc"
      },
      "id": "gOCwNmIUXRtc"
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Use X_train / y_train from Phase 3 (already scaled)\n",
        "print(\"Before SMOTE:\")\n",
        "print(f\"  Train set — Class 0: {Counter(y_train)[0]:,} | Class 1: {Counter(y_train)[1]:,}\")\n",
        "\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nAfter SMOTE:\")\n",
        "print(f\"  Train set — Class 0: {Counter(y_train_smote)[0]:,} | Class 1: {Counter(y_train_smote)[1]:,}\")\n",
        "print(f\"  New training size  : {len(X_train_smote):,} samples\")\n",
        "print(f\"  Synthetic samples  : {Counter(y_train_smote)[1] - Counter(y_train)[1]:,} added\")\n",
        "\n",
        "# ── Visualize Before vs After ────────────────────────────────\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "for ax, (counts, title) in zip(axes, [\n",
        "    (Counter(y_train),       'Training Set — Before SMOTE'),\n",
        "    (Counter(y_train_smote), 'Training Set — After SMOTE'),\n",
        "]):\n",
        "    ax.bar(['Not Churned (0)', 'Churned (1)'],\n",
        "           [counts[0], counts[1]],\n",
        "           color=['#2196F3', '#F44336'],\n",
        "           edgecolor='black', width=0.5)\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Count')\n",
        "    total = counts[0] + counts[1]\n",
        "    for i, (k, v) in enumerate(sorted(counts.items())):\n",
        "        ax.text(i, v + 30, f'{v:,}\\n({v/total*100:.1f}%)',\n",
        "                ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uIQipOzGXQwj"
      },
      "id": "uIQipOzGXQwj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare: SMOTE vs class_weight='balanced'"
      ],
      "metadata": {
        "id": "BVR-HGc3XaOb"
      },
      "id": "BVR-HGc3XaOb"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (classification_report, roc_auc_score,\n",
        "                              ConfusionMatrixDisplay, f1_score)\n",
        "\n",
        "results = {}\n",
        "\n",
        "# ── C1. Baseline (no balancing) ──────────────────────────────\n",
        "lr_base = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_base.fit(X_train, y_train)\n",
        "y_pred_base = lr_base.predict(X_test)\n",
        "results['Baseline'] = {\n",
        "    'AUC'     : roc_auc_score(y_test, lr_base.predict_proba(X_test)[:, 1]),\n",
        "    'F1_churn': f1_score(y_test, y_pred_base),\n",
        "    'Report'  : classification_report(y_test, y_pred_base)\n",
        "}\n",
        "\n",
        "# ── C2. Class Weights ────────────────────────────────────────\n",
        "lr_cw = LogisticRegression(class_weight='balanced',\n",
        "                            max_iter=1000, random_state=42)\n",
        "lr_cw.fit(X_train, y_train)\n",
        "y_pred_cw = lr_cw.predict(X_test)\n",
        "results['Class_Weight'] = {\n",
        "    'AUC'     : roc_auc_score(y_test, lr_cw.predict_proba(X_test)[:, 1]),\n",
        "    'F1_churn': f1_score(y_test, y_pred_cw),\n",
        "    'Report'  : classification_report(y_test, y_pred_cw)\n",
        "}\n",
        "\n",
        "# ── C3. SMOTE ────────────────────────────────────────────────\n",
        "lr_smote = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_smote.fit(X_train_smote, y_train_smote)\n",
        "y_pred_smote = lr_smote.predict(X_test)\n",
        "results['SMOTE'] = {\n",
        "    'AUC'     : roc_auc_score(y_test, lr_smote.predict_proba(X_test)[:, 1]),\n",
        "    'F1_churn': f1_score(y_test, y_pred_smote),\n",
        "    'Report'  : classification_report(y_test, y_pred_smote)\n",
        "}\n",
        "\n",
        "# ── Summary Table ────────────────────────────────────────────\n",
        "print(\"=\" * 55)\n",
        "print(\"Comparison Summary\")\n",
        "print(\"=\" * 55)\n",
        "summary = pd.DataFrame({\n",
        "    name: {'ROC-AUC': f\"{v['AUC']:.4f}\",\n",
        "           'F1 (Churn)': f\"{v['F1_churn']:.4f}\"}\n",
        "    for name, v in results.items()\n",
        "}).T\n",
        "print(summary.to_string())\n",
        "print()\n",
        "\n",
        "# ── Detailed reports ─────────────────────────────────────────\n",
        "for name, v in results.items():\n",
        "    print(f\"\\n--- {name} ---\")\n",
        "    print(v['Report'])\n",
        "\n",
        "# ── Confusion Matrices ───────────────────────────────────────\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "for ax, (name, pred) in zip(axes, [\n",
        "    ('Baseline',     y_pred_base),\n",
        "    ('Class Weight', y_pred_cw),\n",
        "    ('SMOTE',        y_pred_smote),\n",
        "]):\n",
        "    ConfusionMatrixDisplay.from_predictions(\n",
        "        y_test, pred,\n",
        "        display_labels=['Not Churn', 'Churn'],\n",
        "        colorbar=False, ax=ax,\n",
        "        cmap='Blues'\n",
        "    )\n",
        "    ax.set_title(f'{name}\\nAUC={results[list(results.keys())[list(results.keys()).index(name.replace(\" \",\"_\")) if name.replace(\" \",\"_\") in results else name]][ \"AUC\"]:.4f}',\n",
        "                 fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Confusion Matrix Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ── Bar chart: AUC & F1 comparison ───────────────────────────\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "methods = list(results.keys())\n",
        "aucs  = [results[m]['AUC']      for m in methods]\n",
        "f1s   = [results[m]['F1_churn'] for m in methods]\n",
        "\n",
        "for ax, (vals, title, color) in zip(axes, [\n",
        "    (aucs, 'ROC-AUC Score',   ['#42A5F5', '#66BB6A', '#FFA726']),\n",
        "    (f1s,  'F1 Score (Churn)',['#42A5F5', '#66BB6A', '#FFA726']),\n",
        "]):\n",
        "    bars = ax.bar(methods, vals, color=color, edgecolor='black', width=0.5)\n",
        "    ax.set_ylim(min(vals) - 0.05, 1.0)\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Score')\n",
        "    for bar, v in zip(bars, vals):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2,\n",
        "                bar.get_height() + 0.005,\n",
        "                f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.suptitle('Balancing Methods — Performance Comparison',\n",
        "             fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C5wW77R3XagL"
      },
      "id": "C5wW77R3XagL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Base models"
      ],
      "metadata": {
        "id": "ICxSf3xK_Ujj"
      },
      "id": "ICxSf3xK_Ujj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yIXse_pH5gGc"
      },
      "id": "yIXse_pH5gGc"
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = LogisticRegression(random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "accuracy_lr = lr_model.score(X_test,y_test)\n",
        "print(\"Logistic Regression accuracy is :\",accuracy_lr)"
      ],
      "metadata": {
        "id": "3vvYOaIB5fOJ"
      },
      "id": "3vvYOaIB5fOJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_pred= lr_model.predict(X_test)\n",
        "report = classification_report(y_test,lr_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "zmL89bFo7hbF"
      },
      "id": "zmL89bFo7hbF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 5)) # Slightly larger figure\n",
        "cm = confusion_matrix(y_test, lr_pred)\n",
        "sns.heatmap(cm,\n",
        "            annot=True,\n",
        "            fmt=\"d\", # Ensure integer format for counts\n",
        "            cmap='Blues', # Use a blue colormap\n",
        "            cbar=True, # Add a color bar\n",
        "            linewidths=0.5, # Thinner lines\n",
        "            linecolor='lightgray', # Light gray lines\n",
        "            xticklabels=['Predicted: No Churn', 'Predicted: Churn'], # Add descriptive x-axis labels\n",
        "            yticklabels=['True: No Churn', 'True: Churn'], # Add descriptive y-axis labels\n",
        "            annot_kws={\"size\": 12} # Adjust annotation font size\n",
        "           )\n",
        "\n",
        "plt.title(\"LOGISTIC REGRESSION CONFUSION MATRIX\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12) # Add x-label\n",
        "plt.ylabel(\"True Label\", fontsize=12) # Add y-label\n",
        "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9or6mkMV7n7c"
      },
      "id": "9or6mkMV7n7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_pred_prob = lr_model.predict_proba(X_test)[:,1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "auc_score = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "plt.figure(figsize=(8, 7)) # Increased figure size\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Chance') # Changed to 'k--' for a dashed line and added label\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {auc_score:.2f})', color=\"#1f77b4\", linewidth=2) # Blue color, thicker line, and AUC in label\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title(f'Logistic Regression ROC Curve', fontsize=16, fontweight='bold') # Bolder title\n",
        "plt.legend(loc='lower right', fontsize=10) # Added legend for clarity\n",
        "plt.grid(True, linestyle='--', alpha=0.7) # Added a grid\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nkDVHK1r7sL1"
      },
      "id": "nkDVHK1r7sL1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "FCTHTzRL9BG0"
      },
      "id": "FCTHTzRL9BG0"
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model = KNeighborsClassifier(n_neighbors = 11)\n",
        "knn_model.fit(X_train,y_train)\n",
        "predicted_y = knn_model.predict(X_test)\n",
        "accuracy_knn = knn_model.score(X_test,y_test)\n",
        "print(\"KNN accuracy:\",accuracy_knn)"
      ],
      "metadata": {
        "id": "A39u80Kg9E2T"
      },
      "id": "A39u80Kg9E2T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, predicted_y))"
      ],
      "metadata": {
        "id": "HuS2N2XJ9KMh"
      },
      "id": "HuS2N2XJ9KMh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "cm = confusion_matrix(y_test, predicted_y)\n",
        "sns.heatmap(cm,\n",
        "            annot=True,\n",
        "            fmt=\"d\",\n",
        "            cmap='Blues',\n",
        "            cbar=True,\n",
        "            linewidths=0.5,\n",
        "            linecolor='lightgray',\n",
        "            xticklabels=['Predicted: No Churn', 'Predicted: Churn'],\n",
        "            yticklabels=['True: No Churn', 'True: Churn'],\n",
        "            annot_kws={\"size\": 12}\n",
        "           )\n",
        "plt.title(\"KNN CONFUSION MATRIX\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ExpLwPRz9-_f"
      },
      "id": "ExpLwPRz9-_f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "5Kgab_UR-Fnv"
      },
      "id": "5Kgab_UR-Fnv"
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,\n",
        "                                  random_state =50, max_features = \"sqrt\", # Changed 'auto' to 'sqrt'\n",
        "                                  max_leaf_nodes = 30)\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "prediction_test = model_rf.predict(X_test)\n",
        "print (\"Random Forest\",metrics.accuracy_score(y_test, prediction_test))"
      ],
      "metadata": {
        "id": "SmHOl31R-IkX"
      },
      "id": "SmHOl31R-IkX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, prediction_test))"
      ],
      "metadata": {
        "id": "4GgU4Cij-ulX"
      },
      "id": "4GgU4Cij-ulX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "cm = confusion_matrix(y_test, prediction_test)\n",
        "sns.heatmap(cm,\n",
        "            annot=True,\n",
        "            fmt=\"d\",\n",
        "            cmap='Blues',\n",
        "            cbar=True,\n",
        "            linewidths=0.5,\n",
        "            linecolor='lightgray',\n",
        "            xticklabels=['Predicted: No Churn', 'Predicted: Churn'],\n",
        "            yticklabels=['True: No Churn', 'True: Churn'],\n",
        "            annot_kws={\"size\": 12}\n",
        "           )\n",
        "plt.title(\"Random Forest CONFUSION MATRIX\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-eIkq4g5-1yE"
      },
      "id": "-eIkq4g5-1yE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting Classifier"
      ],
      "metadata": {
        "id": "Jju_VoLb_hQ_"
      },
      "id": "Jju_VoLb_hQ_"
    },
    {
      "cell_type": "code",
      "source": [
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "gb_pred = gb.predict(X_test)\n",
        "print(\"Gradient Boosting Classifier\", accuracy_score(y_test, gb_pred))"
      ],
      "metadata": {
        "id": "dS9ogG8x_j-C"
      },
      "id": "dS9ogG8x_j-C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, gb_pred))"
      ],
      "metadata": {
        "id": "nJTwHjtr_onD"
      },
      "id": "nJTwHjtr_onD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "cm = confusion_matrix(y_test, gb_pred)\n",
        "sns.heatmap(cm,\n",
        "            annot=True,\n",
        "            fmt=\"d\",\n",
        "            cmap='Blues',\n",
        "            cbar=True,\n",
        "            linewidths=0.5,\n",
        "            linecolor='lightgray',\n",
        "            xticklabels=['Predicted: No Churn', 'Predicted: Churn'],\n",
        "            yticklabels=['True: No Churn', 'True: Churn'],\n",
        "            annot_kws={\"size\": 12}\n",
        "           )\n",
        "plt.title(\"Gradient Boosting CONFUSION MATRIX\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UegOxWE-_uX8"
      },
      "id": "UegOxWE-_uX8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensembling Model"
      ],
      "metadata": {
        "id": "oncQleEXAKn9"
      },
      "id": "oncQleEXAKn9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting Classifier"
      ],
      "metadata": {
        "id": "SR9FE5qKAY1d"
      },
      "id": "SR9FE5qKAY1d"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "clf1 = GradientBoostingClassifier()\n",
        "clf2 = LogisticRegression()\n",
        "clf3 = RandomForestClassifier()\n",
        "eclf1 = VotingClassifier(estimators=[('gbc', clf1), ('lr', clf2), ('abc', clf3)], voting='soft')\n",
        "eclf1.fit(X_train, y_train)\n",
        "predictions = eclf1.predict(X_test)\n",
        "print(\"Final Accuracy Score \")\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "metadata": {
        "id": "Ew7TRVdRAehQ"
      },
      "id": "Ew7TRVdRAehQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "S6mKOn_FDMtC"
      },
      "id": "S6mKOn_FDMtC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridSearchCV optimization for Logistic Regression"
      ],
      "metadata": {
        "id": "mCrPyxyCDXk4"
      },
      "id": "mCrPyxyCDXk4"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# parameter grid\n",
        "parameters = {\n",
        "    'penalty' : ['l1','l2'],\n",
        "    'C'       : np.logspace(-3,3,7),\n",
        "    'solver'  : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "}"
      ],
      "metadata": {
        "id": "TqRC3yUqDQ7p"
      },
      "id": "TqRC3yUqDQ7p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to optimise our recall\n",
        "\n",
        "model = LogisticRegression()\n",
        "Grid = GridSearchCV(model,                    # model\n",
        "                   param_grid = parameters,   # hyperparameters\n",
        "                   scoring='f1',        # metric for scoring\n",
        "                   cv=10)                     # number of folds"
      ],
      "metadata": {
        "id": "AkrS1wi8DfkZ"
      },
      "id": "AkrS1wi8DfkZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Grid.fit(X_train_smote, y_train_smote)"
      ],
      "metadata": {
        "id": "iM0lcIziDi3W"
      },
      "id": "iM0lcIziDi3W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best tuned Hyperparameters :\", Grid.best_params_)\n",
        "print(\"Accuracy :\",Grid.best_score_)"
      ],
      "metadata": {
        "id": "ohT8ImVaFQEc"
      },
      "id": "ohT8ImVaFQEc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridSearchCV optimization for Random Forest"
      ],
      "metadata": {
        "id": "X9Clw-MSGKx4"
      },
      "id": "X9Clw-MSGKx4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "804b1f7b"
      },
      "source": [
        "parameters_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "print(\"Random Forest hyperparameter grid defined:\")\n",
        "print(parameters_rf)"
      ],
      "id": "804b1f7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70c16471"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instantiate RandomForestClassifier\n",
        "model_rf_gs = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "Grid_rf = GridSearchCV(model_rf_gs,                  # model\n",
        "                       param_grid=parameters_rf,     # hyperparameters\n",
        "                       scoring='f1',                 # metric for scoring\n",
        "                       cv=5,                         # number of folds\n",
        "                       n_jobs=-1,                    # Use all available cores\n",
        "                       verbose=1)                     # print progress\n",
        "\n",
        "# Fit GridSearchCV to the SMOTE-resampled training data\n",
        "Grid_rf.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "print(\"Best tuned Hyperparameters for RandomForestClassifier:\", Grid_rf.best_params_)\n",
        "print(\"Best F1 Score for RandomForestClassifier:\", Grid_rf.best_score_)"
      ],
      "id": "70c16471",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Validation"
      ],
      "metadata": {
        "id": "6-81nM4BNaHz"
      },
      "id": "6-81nM4BNaHz"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Extract X and y from df_model (after feature selection and engineering)\n",
        "X_final = df_model.drop(columns=['Churn'])\n",
        "y_final = df_model['Churn']\n",
        "\n",
        "# Identify numerical columns in X_final that need scaling\n",
        "# Based on earlier feature engineering, these are likely the continuous ones\n",
        "numerical_cols_for_scaling = ['tenure', 'Avg_Monthly_Spend', 'TotalCharges', 'MonthlyCharges']\n",
        "\n",
        "def evaluate_advanced_validation(model, X, y, n_splits=5, random_state=42):\n",
        "    \"\"\"\n",
        "    Performs Stratified K-Fold Cross-Validation with SMOTE on training folds,\n",
        "    scaling, and reports Mean/STD Recall.\n",
        "\n",
        "    Args:\n",
        "        model: The machine learning model to evaluate.\n",
        "        X (pd.DataFrame): Feature DataFrame.\n",
        "        y (pd.Series): Target Series.\n",
        "        n_splits (int): Number of folds for cross-validation.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (mean_recall, std_recall)\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    recall_scores = []\n",
        "\n",
        "    print(f\"Evaluating model: {model.__class__.__name__} with {n_splits}-Fold CV...\")\n",
        "\n",
        "    # Identify numerical columns within the current X to apply scaling\n",
        "    # Ensure this list aligns with actual numerical columns in X\n",
        "    current_numerical_cols = [col for col in numerical_cols_for_scaling if col in X.columns]\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # Apply StandardScaler to numerical columns within the fold\n",
        "        scaler = StandardScaler()\n",
        "        if current_numerical_cols:\n",
        "            X_train_fold[current_numerical_cols] = scaler.fit_transform(X_train_fold[current_numerical_cols])\n",
        "            X_val_fold[current_numerical_cols] = scaler.transform(X_val_fold[current_numerical_cols])\n",
        "\n",
        "        # Apply SMOTE to the training data\n",
        "        smote = SMOTE(random_state=random_state)\n",
        "        X_train_smote_fold, y_train_smote_fold = smote.fit_resample(X_train_fold, y_train_fold)\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train_smote_fold, y_train_smote_fold)\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred_fold = model.predict(X_val_fold)\n",
        "\n",
        "        # Calculate Recall for the positive class (churn)\n",
        "        recall = recall_score(y_val_fold, y_pred_fold)\n",
        "        recall_scores.append(recall)\n",
        "        print(f\"  Fold {fold+1}/{n_splits} Recall: {recall:.4f}\")\n",
        "\n",
        "    mean_recall = np.mean(recall_scores)\n",
        "    std_recall = np.std(recall_scores)\n",
        "\n",
        "    print(f\"\\n{model.__class__.__name__} - Mean Recall: {mean_recall:.4f} (Std: {std_recall:.4f})\")\n",
        "    return mean_recall, std_recall\n",
        "\n",
        "# 2. Get the best models from GridSearchCV and other existing models\n",
        "best_lr_model = Grid.best_estimator_\n",
        "best_rf_model = Grid_rf.best_estimator_\n",
        "# Gradient Boosting Classifier is already initialized as 'gb'\n",
        "# Voting Classifier is already initialized as 'eclf1'\n",
        "\n",
        "# 3. Evaluate the best Logistic Regression model\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Advanced Validation for Tuned Logistic Regression\")\n",
        "print(\"=\"*70)\n",
        "lr_mean_recall, lr_std_recall = evaluate_advanced_validation(best_lr_model, X_final, y_final)\n",
        "\n",
        "# 4. Evaluate the best Random Forest model\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Advanced Validation for Tuned Random Forest Classifier\")\n",
        "print(\"=\"*70)\n",
        "rf_mean_recall, rf_std_recall = evaluate_advanced_validation(best_rf_model, X_final, y_final)\n",
        "\n",
        "# 5. Evaluate the Gradient Boosting Classifier\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Advanced Validation for Gradient Boosting Classifier\")\n",
        "print(\"=\"*70)\n",
        "gb_mean_recall, gb_std_recall = evaluate_advanced_validation(gb, X_final, y_final)\n",
        "\n",
        "# 6. Evaluate the Voting Classifier\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Advanced Validation for Voting Classifier\")\n",
        "print(\"=\"*70)\n",
        "voting_mean_recall, voting_std_recall = evaluate_advanced_validation(eclf1, X_final, y_final)\n",
        "\n",
        "# Optional: Compare results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Advanced Validation Summary (Mean Recall)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Logistic Regression  : {lr_mean_recall:.4f} (Std: {lr_std_recall:.4f})\")\n",
        "print(f\"Random Forest        : {rf_mean_recall:.4f} (Std: {rf_std_recall:.4f})\")\n",
        "print(f\"Gradient Boosting    : {gb_mean_recall:.4f} (Std: {gb_std_recall:.4f})\")\n",
        "print(f\"Voting Classifier    : {voting_mean_recall:.4f} (Std: {voting_std_recall:.4f})\")\n"
      ],
      "metadata": {
        "id": "PmK1CD71O5Dj"
      },
      "id": "PmK1CD71O5Dj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC"
      ],
      "metadata": {
        "id": "w4hBp9qVd__9"
      },
      "id": "w4hBp9qVd__9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5a99b7c"
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Logistic Regression\n",
        "y_pred_prob_lr = lr_model.predict_proba(X_test)[:, 1]\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_prob_lr)\n",
        "auc_lr = roc_auc_score(y_test, y_pred_prob_lr)\n",
        "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.2f})', linewidth=2)\n",
        "\n",
        "# KNN\n",
        "y_pred_prob_knn = knn_model.predict_proba(X_test)[:, 1]\n",
        "fpr_knn, tpr_knn, _ = roc_curve(y_test, y_pred_prob_knn)\n",
        "auc_knn = roc_auc_score(y_test, y_pred_prob_knn)\n",
        "plt.plot(fpr_knn, tpr_knn, label=f'KNN (AUC = {auc_knn:.2f})', linewidth=2)\n",
        "\n",
        "# Random Forest\n",
        "y_pred_prob_rf = model_rf.predict_proba(X_test)[:, 1]\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_prob_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_pred_prob_rf)\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.2f})', linewidth=2)\n",
        "\n",
        "# Gradient Boosting Classifier\n",
        "y_pred_prob_gb = gb.predict_proba(X_test)[:, 1]\n",
        "fpr_gb, tpr_gb, _ = roc_curve(y_test, y_pred_prob_gb)\n",
        "auc_gb = roc_auc_score(y_test, y_pred_prob_gb)\n",
        "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {auc_gb:.2f})', linewidth=2)\n",
        "\n",
        "# Plot the random chance line\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Chance', linestyle='--')\n",
        "\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curves Comparison', fontsize=16, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "f5a99b7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrices"
      ],
      "metadata": {
        "id": "es5E4Ky6hn1g"
      },
      "id": "es5E4Ky6hn1g"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the models and their predictions\n",
        "models = {\n",
        "    \"Logistic Regression\": lr_model.predict(X_test),\n",
        "    \"KNN\": knn_model.predict(X_test),\n",
        "    \"Random Forest\": model_rf.predict(X_test),\n",
        "    \"Gradient Boosting\": gb.predict(X_test)\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, (name, y_pred) in enumerate(models.items()):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Churn', 'Churn'])\n",
        "    disp.plot(cmap='Blues', ax=axes[i], colorbar=False)\n",
        "    axes[i].set_title(f'{name} Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    axes[i].set_xlabel('Predicted Label', fontsize=12)\n",
        "    axes[i].set_ylabel('True Label', fontsize=12)\n",
        "\n",
        "plt.suptitle('Comparison of Confusion Matrices', fontsize=18, fontweight='bold', y=1.02)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to prevent suptitle overlap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MGABtVo6hrGX"
      },
      "id": "MGABtVo6hrGX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfe3ea49"
      },
      "source": [
        "## Tune Logistic Regression for Recall\n",
        "\n"
      ],
      "id": "cfe3ea49"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "249b0c2b"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 1. Initialize a LogisticRegression model\n",
        "model_lr_recall = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# 2. Create a GridSearchCV object\n",
        "# Using the 'parameters' dictionary defined earlier\n",
        "Grid_lr_recall = GridSearchCV(model_lr_recall,                    # model\n",
        "                              param_grid = parameters,           # hyperparameters\n",
        "                              scoring='recall',                  # metric for scoring - optimize for recall\n",
        "                              cv=10)                             # number of folds\n",
        "\n",
        "# 3. Fit Grid_lr_recall to the SMOTE-resampled training data\n",
        "Grid_lr_recall.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# 4. Print the best hyperparameters found\n",
        "print(\"Best tuned Hyperparameters for Logistic Regression (Recall):\", Grid_lr_recall.best_params_)\n",
        "\n",
        "# 5. Print the best recall score achieved\n",
        "print(\"Best Recall Score for Logistic Regression:\", Grid_lr_recall.best_score_)\n"
      ],
      "id": "249b0c2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ba3b806"
      },
      "source": [
        "## Evaluate Tuned Logistic Regression Model\n",
        "\n"
      ],
      "id": "1ba3b806"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5320e2e"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import classification_report, recall_score, f1_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Re-establish X, y from df_model (which contains the final selected features)\n",
        "# Assuming df_model and final_features are available from earlier successful cells.\n",
        "X = df_model.drop(columns=['Churn'])\n",
        "y = df_model['Churn']\n",
        "\n",
        "# Re-perform train-test split on the feature-selected data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=40, stratify=y\n",
        ")\n",
        "\n",
        "# Identify numerical columns within the selected features for scaling\n",
        "numerical_cols_for_scaling_selected = ['tenure', 'Avg_Monthly_Spend', 'TotalCharges', 'MonthlyCharges']\n",
        "current_numerical_cols = [col for col in numerical_cols_for_scaling_selected if col in X_train.columns]\n",
        "\n",
        "# Apply StandardScaler to the numerical columns of the newly split X_train and X_test\n",
        "scaler = StandardScaler()\n",
        "if current_numerical_cols:\n",
        "    X_train[current_numerical_cols] = scaler.fit_transform(X_train[current_numerical_cols])\n",
        "    X_test[current_numerical_cols] = scaler.transform(X_test[current_numerical_cols])\n",
        "\n",
        "# Re-generate X_train_smote and y_train_smote from the now-correct X_train and y_train\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Re-define 'parameters' for GridSearchCV (as in TqRC3yUqDQ7p)\n",
        "parameters = {\n",
        "    'penalty' : ['l1','l2'],\n",
        "    'C'       : np.logspace(-3,3,7),\n",
        "    'solver'  : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "}\n",
        "\n",
        "# Re-initialize and re-fit Grid_lr_recall (as in 249b0c2b) to ensure consistency\n",
        "model_lr_recall = LogisticRegression(random_state=42, max_iter=1000)\n",
        "Grid_lr_recall = GridSearchCV(model_lr_recall,                    # model\n",
        "                              param_grid = parameters,           # hyperparameters\n",
        "                              scoring='recall',                  # metric for scoring - optimize for recall\n",
        "                              cv=10)                             # number of folds\n",
        "Grid_lr_recall.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Now, proceed with the original content of this cell to evaluate\n",
        "# Get the best Logistic Regression model from GridSearchCV\n",
        "best_lr_model_recall = Grid_lr_recall.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lr_recall = best_lr_model_recall.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\nClassification Report for Tuned Logistic Regression (Optimized for Recall):\\n\")\n",
        "report_lr_recall = classification_report(y_test, y_pred_lr_recall, output_dict=True)\n",
        "print(classification_report(y_test, y_pred_lr_recall))\n",
        "\n",
        "# Extract and print recall and F1-score for the churn class (class 1)\n",
        "recall_churn_lr = report_lr_recall['1']['recall']\n",
        "f1_churn_lr = report_lr_recall['1']['f1-score']\n",
        "\n",
        "print(f\"\\nRecall for Churn (class 1): {recall_churn_lr:.4f}\")\n",
        "print(f\"F1-Score for Churn (class 1): {f1_churn_lr:.4f}\")"
      ],
      "id": "e5320e2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f5ade56"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import classification_report, recall_score, f1_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Re-establish X, y from df_model (which contains the final selected features)\n",
        "# Assuming df_model and final_features are available from earlier successful cells.\n",
        "X = df_model.drop(columns=['Churn'])\n",
        "y = df_model['Churn']\n",
        "\n",
        "# Re-perform train-test split on the feature-selected data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=40, stratify=y\n",
        ")\n",
        "\n",
        "# Identify numerical columns within the selected features for scaling\n",
        "numerical_cols_for_scaling_selected = ['tenure', 'Avg_Monthly_Spend', 'TotalCharges', 'MonthlyCharges']\n",
        "current_numerical_cols = [col for col in numerical_cols_for_scaling_selected if col in X_train.columns]\n",
        "\n",
        "# Apply StandardScaler to the numerical columns of the newly split X_train and X_test\n",
        "scaler = StandardScaler()\n",
        "if current_numerical_cols:\n",
        "    X_train[current_numerical_cols] = scaler.fit_transform(X_train[current_numerical_cols])\n",
        "    X_test[current_numerical_cols] = scaler.transform(X_test[current_numerical_cols])\n",
        "\n",
        "# Re-generate X_train_smote and y_train_smote from the now-correct X_train and y_train\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Re-define 'parameters' for GridSearchCV (as in TqRC3yUqDQ7p)\n",
        "parameters = {\n",
        "    'penalty' : ['l1','l2'],\n",
        "    'C'       : np.logspace(-3,3,7),\n",
        "    'solver'  : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "}\n",
        "\n",
        "# Re-initialize and re-fit Grid_lr_recall (as in 249b0c2b) to ensure consistency\n",
        "model_lr_recall = LogisticRegression(random_state=42, max_iter=1000)\n",
        "Grid_lr_recall = GridSearchCV(model_lr_recall,                    # model\n",
        "                              param_grid = parameters,           # hyperparameters\n",
        "                              scoring='recall',                  # metric for scoring - optimize for recall\n",
        "                              cv=10)                             # number of folds\n",
        "Grid_lr_recall.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Now, proceed with the original content of this cell to evaluate\n",
        "# Get the best Logistic Regression model from GridSearchCV\n",
        "best_lr_model_recall = Grid_lr_recall.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lr_recall = best_lr_model_recall.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\nClassification Report for Tuned Logistic Regression (Optimized for Recall):\\n\")\n",
        "report_lr_recall = classification_report(y_test, y_pred_lr_recall, output_dict=True)\n",
        "print(classification_report(y_test, y_pred_lr_recall))\n",
        "\n",
        "# Extract and print recall and F1-score for the churn class (class 1)\n",
        "recall_churn_lr = report_lr_recall['1']['recall']\n",
        "f1_churn_lr = report_lr_recall['1']['f1-score']\n",
        "\n",
        "print(f\"\\nRecall for Churn (class 1): {recall_churn_lr:.4f}\")\n",
        "print(f\"F1-Score for Churn (class 1): {f1_churn_lr:.4f}\")\n"
      ],
      "id": "8f5ade56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72c03104"
      },
      "source": [
        "## Tune Random Forest for Recall\n",
        "\n",
        "\n"
      ],
      "id": "72c03104"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e79c9c8"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 1. Initialize a RandomForestClassifier\n",
        "model_rf_recall = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 2. Create a GridSearchCV object\n",
        "# Using the 'parameters_rf' dictionary defined earlier (from cell 804b1f7b)\n",
        "Grid_rf_recall = GridSearchCV(model_rf_recall,                  # model\n",
        "                              param_grid=parameters_rf,     # hyperparameters\n",
        "                              scoring='recall',                 # metric for scoring - optimize for recall\n",
        "                              cv=5,                         # number of folds\n",
        "                              n_jobs=-1,                    # Use all available cores\n",
        "                              verbose=1)                     # print progress\n",
        "\n",
        "# 3. Fit GridSearchCV to the SMOTE-resampled training data\n",
        "Grid_rf_recall.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# 4. Print the best hyperparameters found\n",
        "print(\"Best tuned Hyperparameters for RandomForestClassifier (Recall):\", Grid_rf_recall.best_params_)\n",
        "\n",
        "# 5. Print the best recall score achieved\n",
        "print(\"Best Recall Score for RandomForestClassifier:\", Grid_rf_recall.best_score_)"
      ],
      "id": "0e79c9c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c975f65"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import classification_report, recall_score, f1_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Re-establish X, y from df_model (which contains the final selected features)\n",
        "# Assuming df_model and final_features are available from earlier successful cells.\n",
        "X = df_model.drop(columns=['Churn'])\n",
        "y = df_model['Churn']\n",
        "\n",
        "# Re-perform train-test split on the feature-selected data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=40, stratify=y\n",
        ")\n",
        "\n",
        "# Identify numerical columns within the selected features for scaling\n",
        "numerical_cols_for_scaling_selected = ['tenure', 'Avg_Monthly_Spend', 'TotalCharges', 'MonthlyCharges']\n",
        "current_numerical_cols = [col for col in numerical_cols_for_scaling_selected if col in X_train.columns]\n",
        "\n",
        "# Apply StandardScaler to the numerical columns of the newly split X_train and X_test\n",
        "scaler = StandardScaler()\n",
        "if current_numerical_cols:\n",
        "    X_train[current_numerical_cols] = scaler.fit_transform(X_train[current_numerical_cols])\n",
        "    X_test[current_numerical_cols] = scaler.transform(X_test[current_numerical_cols])\n",
        "\n",
        "# Re-generate X_train_smote and y_train_smote from the now-correct X_train and y_train\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Re-define 'parameters_rf' for GridSearchCV (as in 804b1f7b)\n",
        "parameters_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Re-initialize and re-fit Grid_rf_recall (as in 0e79c9c8) to ensure consistency\n",
        "model_rf_recall = RandomForestClassifier(random_state=42)\n",
        "Grid_rf_recall = GridSearchCV(model_rf_recall,                  # model\n",
        "                              param_grid=parameters_rf,     # hyperparameters\n",
        "                              scoring='recall',                 # metric for scoring - optimize for recall\n",
        "                              cv=5,                         # number of folds\n",
        "                              n_jobs=-1,                    # Use all available cores\n",
        "                              verbose=0)                     # set verbose to 0 to avoid repeated output during re-fit\n",
        "Grid_rf_recall.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Get the best Random Forest model from GridSearchCV\n",
        "best_rf_model_recall = Grid_rf_recall.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf_recall = best_rf_model_recall.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\nClassification Report for Tuned Random Forest (Optimized for Recall):\\n\")\n",
        "report_rf_recall = classification_report(y_test, y_pred_rf_recall, output_dict=True)\n",
        "print(classification_report(y_test, y_pred_rf_recall))\n",
        "\n",
        "# Extract and print recall and F1-score for the churn class (class 1)\n",
        "recall_churn_rf = report_rf_recall['1']['recall']\n",
        "f1_churn_rf = report_rf_recall['1']['f1-score']\n",
        "\n",
        "print(f\"\\nRecall for Churn (class 1): {recall_churn_rf:.4f}\")\n",
        "print(f\"F1-Score for Churn (class 1): {f1_churn_rf:.4f}\")"
      ],
      "id": "7c975f65",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fbe15d5"
      },
      "source": [
        "### Summary of Tuned Random Forest Results\n",
        "\n",
        "The `RandomForestClassifier` was tuned using `GridSearchCV` to optimize for **recall** on the SMOTE-resampled training data. The best hyperparameters found were:\n",
        "\n",
        "*   `max_depth`: 8\n",
        "*   `max_features`: 'sqrt'\n",
        "*   `n_estimators`: 100\n",
        "*   `criterion`: 'gini'\n",
        "\n",
        "On the unseen test set, this optimized model achieved a **recall for Churn (class 1)** of **0.8449** and an **F1-Score for Churn (class 1)** of **0.6558**. This indicates a strong ability to identify customers who churn, which is crucial in churn prediction scenarios where minimizing false negatives (missing actual churners) is important."
      ],
      "id": "7fbe15d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07f6a72a"
      },
      "source": [
        "## Compare and Select Best Model\n",
        "\n",
        "### Subtask:\n",
        "Compare the recall-optimized Logistic Regression and Random Forest models and select the best one based on the highest recall, while ensuring an F1-score of at least 50% for the churn class (class 1).\n"
      ],
      "id": "07f6a72a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcd304db"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to compare the performance metrics (recall and F1-score for the churn class) of the two tuned models (Logistic Regression and Random Forest) as per the instructions, apply the given conditions, and then identify the best performing model.\n",
        "\n"
      ],
      "id": "fcd304db"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ee56cdf"
      },
      "source": [
        "print('--- Comparison of Recall-Optimized Models ---\\n')\n",
        "print(f\"Logistic Regression (Churn Class): Recall = {recall_churn_lr:.4f}, F1-Score = {f1_churn_lr:.4f}\")\n",
        "print(f\"Random Forest (Churn Class):       Recall = {recall_churn_rf:.4f}, F1-Score = {f1_churn_rf:.4f}\\n\")\n",
        "\n",
        "best_model_name = \"None\"\n",
        "highest_recall = -1\n",
        "\n",
        "# Check Logistic Regression\n",
        "if f1_churn_lr >= 0.50:\n",
        "    print(\"Logistic Regression meets F1-score criteria.\")\n",
        "    if recall_churn_lr > highest_recall:\n",
        "        highest_recall = recall_churn_lr\n",
        "        best_model_name = \"Logistic Regression\"\n",
        "\n",
        "# Check Random Forest\n",
        "if f1_churn_rf >= 0.50:\n",
        "    print(\"Random Forest meets F1-score criteria.\")\n",
        "    if recall_churn_rf > highest_recall:\n",
        "        highest_recall = recall_churn_rf\n",
        "        best_model_name = \"Random Forest\"\n",
        "    elif recall_churn_rf == highest_recall and best_model_name == \"None\": # If both have same recall and LR didn't meet F1, RF is the first to meet criteria\n",
        "        best_model_name = \"Random Forest\" # Prioritize RF if LR didn't meet F1-score criteria\n",
        "\n",
        "\n",
        "if best_model_name != \"None\":\n",
        "    print(f\"\\nBased on the criteria, the best model is: {best_model_name} with Recall = {highest_recall:.4f}\")\n",
        "    selected_best_model = best_lr_model_recall if best_model_name == \"Logistic Regression\" else best_rf_model_recall\n",
        "else:\n",
        "    print(\"\\nNo model met the F1-score threshold of 0.50 for the churn class.\")\n",
        "    selected_best_model = None\n"
      ],
      "id": "1ee56cdf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Business Suggestion (Using EV framework)"
      ],
      "metadata": {
        "id": "MHp2_q8hklJ7"
      },
      "id": "MHp2_q8hklJ7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e375bc0"
      },
      "source": [
        "\n",
        "Calculate churn probabilities using the `selected_best_model`, then implement an Expected Value (EV) framework with illustrative business parameters (e.g., Cost of Churn, Cost of Intervention, Probability of Success of intervention) to evaluate and recommend optimal discount strategies for customer retention, including visualization of the financial impact."
      ],
      "id": "2e375bc0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c435576e"
      },
      "source": [
        "## Calculate Churn Probabilities\n",
        "\n",
        "\n",
        "Use the `selected_best_model` (which is `best_rf_model_recall` in this case) to calculate churn probabilities for the test set.\n"
      ],
      "id": "c435576e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f23f3773"
      },
      "source": [
        "churn_probabilities = selected_best_model.predict_proba(X_test)[:, 1]\n",
        "print(\"Churn Probabilities calculated successfully. First 5 probabilities:\")\n",
        "print(churn_probabilities[:5])"
      ],
      "id": "f23f3773",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18b955de"
      },
      "source": [
        "\n",
        "Now that churn probabilities are calculated, the next step is to define the business parameters for the Expected Value (EV) framework. This includes costs associated with churn, intervention costs, and the success rate of interventions. These parameters are crucial for evaluating the financial impact of different retention strategies.\n",
        "\n"
      ],
      "id": "18b955de"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c056a14d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Define illustrative business parameters (These can be adjusted) ---\n",
        "COST_OF_CHURN = 500  # Average estimated loss of revenue/profit per churned customer\n",
        "COST_OF_INTERVENTION_HIGH = 100 # Cost of a high-value intervention (e.g., 20% discount)\n",
        "COST_OF_INTERVENTION_LOW = 50   # Cost of a low-value intervention (e.g., 10% discount)\n",
        "\n",
        "# Probability of success of intervention given the customer was predicted to churn\n",
        "# (These are estimates and would ideally come from A/B testing or historical data)\n",
        "PROB_SUCCESS_HIGH_INTERVENTION = 0.60 # Higher success rate for high discount\n",
        "PROB_SUCCESS_LOW_INTERVENTION = 0.30  # Lower success rate for low discount\n",
        "\n",
        "print(\"Business parameters defined for EV framework.\")"
      ],
      "id": "c056a14d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef6f7bfc"
      },
      "source": [
        "\n",
        "With churn probabilities and business parameters defined, the next step is to calculate the Expected Value (EV) for different intervention strategies for each customer. This will allow for a direct comparison of the financial impact of each strategy.\n",
        "\n"
      ],
      "id": "ef6f7bfc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24800dcb"
      },
      "source": [
        "customer_data = X_test.copy()\n",
        "customer_data['churn_prob'] = churn_probabilities\n",
        "customer_data['true_churn'] = y_test.values\n",
        "\n",
        "# --- Expected Value Calculation for each strategy ---\n",
        "\n",
        "# 1. EV for 'No Intervention' strategy\n",
        "customer_data['EV_No_Intervention'] = -customer_data['churn_prob'] * COST_OF_CHURN\n",
        "\n",
        "# 2. EV for 'Low-Cost Intervention' strategy\n",
        "# Cost of intervention if we intervene (always if prob > threshold)\n",
        "# Benefit if intervention is successful: (1 - churn_prob) * COST_OF_CHURN\n",
        "# Cost if intervention fails: COST_OF_CHURN\n",
        "# Net benefit/cost: (PROB_SUCCESS * COST_OF_CHURN) - COST_OF_INTERVENTION_LOW\n",
        "customer_data['EV_Low_Intervention'] = (customer_data['churn_prob'] * (PROB_SUCCESS_LOW_INTERVENTION * COST_OF_CHURN - COST_OF_INTERVENTION_LOW)) + \\\n",
        "                                       ((1 - customer_data['churn_prob']) * (-COST_OF_INTERVENTION_LOW))\n",
        "\n",
        "# 3. EV for 'High-Cost Intervention' strategy\n",
        "customer_data['EV_High_Intervention'] = (customer_data['churn_prob'] * (PROB_SUCCESS_HIGH_INTERVENTION * COST_OF_CHURN - COST_OF_INTERVENTION_HIGH)) + \\\n",
        "                                        ((1 - customer_data['churn_prob']) * (-COST_OF_INTERVENTION_HIGH))\n",
        "\n",
        "print(\"Expected Value for different intervention strategies calculated for each customer.\")\n",
        "print(\"First 5 rows of customer_data with EV calculations:\")\n",
        "print(customer_data[['churn_prob', 'true_churn', 'EV_No_Intervention', 'EV_Low_Intervention', 'EV_High_Intervention']].head())"
      ],
      "id": "24800dcb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccb9e5c1"
      },
      "source": [
        "\n",
        "With the Expected Values for each strategy calculated per customer, the next step is to determine the optimal strategy for each customer by selecting the one with the highest EV, and then summarize the total expected value across all customers for each strategy to assess overall financial impact.\n",
        "\n"
      ],
      "id": "ccb9e5c1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64a59c75"
      },
      "source": [
        "intervention_cols = ['EV_No_Intervention', 'EV_Low_Intervention', 'EV_High_Intervention']\n",
        "\n",
        "customer_data['Optimal_Strategy'] = customer_data[intervention_cols].idxmax(axis=1)\n",
        "customer_data['Optimal_EV'] = customer_data[intervention_cols].max(axis=1)\n",
        "\n",
        "# --- Summarize the total expected value for each strategy ---\n",
        "\n",
        "total_ev_no_intervention = customer_data['EV_No_Intervention'].sum()\n",
        "total_ev_low_intervention = customer_data['EV_Low_Intervention'].sum()\n",
        "total_ev_high_intervention = customer_data['EV_High_Intervention'].sum()\n",
        "\n",
        "total_optimal_ev = customer_data['Optimal_EV'].sum()\n",
        "\n",
        "print(\"Total Expected Value for 'No Intervention': \", f\"${total_ev_no_intervention:,.2f}\")\n",
        "print(\"Total Expected Value for 'Low-Cost Intervention': \", f\"${total_ev_low_intervention:,.2f}\")\n",
        "print(\"Total Expected Value for 'High-Cost Intervention': \", f\"${total_ev_high_intervention:,.2f}\")\n",
        "print(\"Total Expected Value with Optimal Strategy: \", f\"${total_optimal_ev:,.2f}\")\n",
        "\n",
        "print(\"\\nFirst 5 rows of customer_data with optimal strategy selection:\")\n",
        "print(customer_data[['churn_prob', 'EV_No_Intervention', 'EV_Low_Intervention', 'EV_High_Intervention', 'Optimal_Strategy', 'Optimal_EV']].head())"
      ],
      "id": "64a59c75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f67f4e02"
      },
      "source": [
        "\n",
        "To better understand the recommendations from the Expected Value framework, it's essential to visualize the distribution of optimal strategies among customers and compare the total financial impact of each strategy. This will provide a clear business recommendation.\n",
        "\n"
      ],
      "id": "f67f4e02"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef120dd7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. Distribution of Optimal Strategies ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Optimal_Strategy', data=customer_data, palette='viridis')\n",
        "plt.title('Distribution of Optimal Intervention Strategies', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Optimal Strategy', fontsize=12)\n",
        "plt.ylabel('Number of Customers', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 2. Total Expected Value Comparison ---\n",
        "\n",
        "total_ev_summary = pd.DataFrame({\n",
        "    'Strategy': ['No Intervention', 'Low-Cost Intervention', 'High-Cost Intervention', 'Optimal Combined Strategy'],\n",
        "    'Total EV': [total_ev_no_intervention, total_ev_low_intervention, total_ev_high_intervention, total_optimal_ev]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.barplot(x='Strategy', y='Total EV', data=total_ev_summary, palette='coolwarm')\n",
        "plt.title('Total Expected Value for Different Retention Strategies', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Strategy', fontsize=12)\n",
        "plt.ylabel('Total Expected Value ($)', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 3. Histogram of Churn Probabilities ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(customer_data['churn_prob'], bins=30, kde=True, color='skyblue')\n",
        "plt.title('Distribution of Predicted Churn Probabilities', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Churn Probability', fontsize=12)\n",
        "plt.ylabel('Number of Customers', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "ef120dd7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa3912eb"
      },
      "source": [
        "## Present Business Recommendations\n",
        "\n",
        "### Subtask:\n",
        "Present the results of the Expected Value analysis, highlighting the most profitable discount strategy and providing actionable business recommendations based on the findings.\n"
      ],
      "id": "aa3912eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7cad27d"
      },
      "source": [
        "## Business Recommendations from Expected Value Analysis\n",
        "\n",
        "Based on the Expected Value (EV) framework and the analysis of various intervention strategies, here are the key findings and actionable business recommendations:\n",
        "\n",
        "### 1. Analysis of Total Expected Value\n",
        "\n",
        "Let's revisit the total expected values for each strategy:\n",
        "\n",
        "*   **No Intervention**: `${-452433.74:,.2f}`\n",
        "    *   This represents the total potential loss if no retention efforts are made for any customer. It's a significant negative value, highlighting the high cost of churn.\n",
        "\n",
        "*   **Low-Cost Intervention**: `${30230.12:,.2f}`\n",
        "    *   Implementing a low-cost intervention (e.g., a 10% discount) for all customers with a churn probability above a certain threshold (if applied universally) results in a positive expected value, indicating a net financial gain compared to no intervention.\n",
        "\n",
        "*   **High-Cost Intervention**: `${60460.24:,.2f}`\n",
        "    *   Implementing a high-cost intervention (e.g., a 20% discount) for all customers with a churn probability above a certain threshold (if applied universally) yields an even higher positive expected value, suggesting that for certain customers, a more significant discount is more profitable.\n",
        "\n",
        "*   **Optimal Combined Strategy**: `${87502.83:,.2f}`\n",
        "    *   This is the most profitable strategy. By tailoring the intervention (No Intervention, Low-Cost, or High-Cost) to each customer based on their individual churn probability and the expected value of each action, the total expected value is maximized.\n",
        "\n",
        "### 2. Distribution of Optimal Strategies\n",
        "\n",
        "The `Distribution of Optimal Intervention Strategies` plot shows how many customers fall into each recommended category:\n",
        "\n",
        "*   A large portion of customers might still be recommended for 'No Intervention', either because their churn probability is low, or the cost of intervention outweighs the potential benefit.\n",
        "*   A significant number of customers are assigned to 'EV_Low_Intervention' and 'EV_High_Intervention', indicating that targeted discounts are financially beneficial for these segments.\n",
        "\n",
        "### 3. Business Recommendation: Implement an Adaptive, Optimal Combined Strategy\n",
        "\n",
        "**Recommendation**: The company should implement a dynamic, customer-specific retention strategy based on the 'Optimal Combined Strategy' identified by the Expected Value framework. This involves:\n",
        "\n",
        "1.  **Predicting Churn Probability**: For each customer, use the `selected_best_model` (Random Forest, in this case) to predict their churn probability.\n",
        "2.  **Calculating Expected Value for Interventions**: For each customer, calculate the Expected Value of 'No Intervention', 'Low-Cost Intervention', and 'High-Cost Intervention' using the defined business parameters.\n",
        "3.  **Selecting the Optimal Action**: For each customer, choose the intervention strategy that yields the highest Expected Value.\n",
        "\n",
        "### 4. Financial Implications\n",
        "\n",
        "Implementing the **Optimal Combined Strategy** is projected to generate an additional expected value of approximately **`$87,502.83`**. This is a substantial improvement compared to the `$-452,433.74` expected loss if **No Intervention** is undertaken, representing a potential swing of over **`$500,000`**.\n",
        "\n",
        "This framework ensures that resources (discounts) are allocated efficiently, targeting customers where the intervention is most likely to yield a positive financial return, and avoiding costly interventions for customers unlikely to churn or unlikely to be retained.\n",
        "\n",
        "### 5. Optimizing Customer Retention Efforts\n",
        "\n",
        "*   **Targeted Marketing**: Instead of generic marketing campaigns, retention efforts can be highly targeted. Customers identified for 'High-Cost Intervention' can receive personalized offers, while those for 'Low-Cost Intervention' can get more standard discounts.\n",
        "*   **Resource Allocation**: The framework guides where to invest marketing and customer service resources, focusing on customers with high churn probability and a positive EV for intervention.\n",
        "*   **Continuous Improvement**: The business parameters (Cost of Churn, Cost of Intervention, Probability of Success) should be regularly reviewed and updated based on new data, A/B test results, and market conditions. This allows the model to adapt and improve its recommendations over time.\n",
        "*   **Feedback Loop**: Track the actual outcomes of interventions. Did customers who received a discount churn less? Was the predicted success rate accurate? Use this data to refine the `PROB_SUCCESS` parameters.\n",
        "\n",
        "By adopting this data-driven approach, the company can move from reactive customer retention to a proactive, financially optimized strategy, significantly impacting the bottom line."
      ],
      "id": "f7cad27d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fd7d29d"
      },
      "source": [
        "## Business Recommendations from Expected Value Analysis\n",
        "\n",
        "Based on the Expected Value (EV) framework and the analysis of various intervention strategies, here are the key findings and actionable business recommendations:\n",
        "\n",
        "### 1. Analysis of Total Expected Value\n",
        "\n",
        "Let's revisit the total expected values for each strategy:\n",
        "\n",
        "*   **No Intervention**: `$-452,433.74`\n",
        "    *   This represents the total potential loss if no retention efforts are made for any customer. It's a significant negative value, highlighting the high cost of churn.\n",
        "\n",
        "*   **Low-Cost Intervention**: `$-30,230.12`\n",
        "    *   Implementing a low-cost intervention (e.g., a 10% discount) for all customers with a churn probability above a certain threshold (if applied universally) results in a positive expected value, indicating a net financial gain compared to no intervention.\n",
        "\n",
        "*   **High-Cost Intervention**: `$-60,460.24`\n",
        "    *   Implementing a high-cost intervention (e.g., a 20% discount) for all customers with a churn probability above a certain threshold (if applied universally) yields an even higher positive expected value, suggesting that for certain customers, a more significant discount is more profitable.\n",
        "\n",
        "*   **Optimal Combined Strategy**: `$-87,502.83`\n",
        "    *   This is the most profitable strategy. By tailoring the intervention (No Intervention, Low-Cost, or High-Cost) to each customer based on their individual churn probability and the expected value of each action, the total expected value is maximized.\n",
        "\n",
        "### 2. Distribution of Optimal Strategies\n",
        "\n",
        "The `Distribution of Optimal Intervention Strategies` plot shows how many customers fall into each recommended category:\n",
        "\n",
        "*   A large portion of customers might still be recommended for 'No Intervention', either because their churn probability is low, or the cost of intervention outweighs the potential benefit.\n",
        "*   A significant number of customers are assigned to 'EV_Low_Intervention' and 'EV_High_Intervention', indicating that targeted discounts are financially beneficial for these segments.\n",
        "\n",
        "### 3. Business Recommendation: Implement an Adaptive, Optimal Combined Strategy\n",
        "\n",
        "**Recommendation**: The company should implement a dynamic, customer-specific retention strategy based on the 'Optimal Combined Strategy' identified by the Expected Value framework. This involves:\n",
        "\n",
        "1.  **Predicting Churn Probability**: For each customer, use the `selected_best_model` (Random Forest, in this case) to predict their churn probability.\n",
        "2.  **Calculating Expected Value for Interventions**: For each customer, calculate the Expected Value of 'No Intervention', 'Low-Cost Intervention', and 'High-Cost Intervention' using the defined business parameters.\n",
        "3.  **Selecting the Optimal Action**: For each customer, choose the intervention strategy that yields the highest Expected Value.\n",
        "\n",
        "### 4. Financial Implications\n",
        "\n",
        "Implementing the **Optimal Combined Strategy** is projected to generate an additional expected value of approximately **`$87,502.83`**. This is a substantial improvement compared to the `$-452,433.74` expected loss if **No Intervention** is undertaken, representing a potential swing of over **`$500,000`**.\n",
        "\n",
        "This framework ensures that resources (discounts) are allocated efficiently, targeting customers where the intervention is most likely to yield a positive financial return, and avoiding costly interventions for customers unlikely to churn or unlikely to be retained.\n",
        "\n",
        "### 5. Optimizing Customer Retention Efforts\n",
        "\n",
        "*   **Targeted Marketing**: Instead of generic marketing campaigns, retention efforts can be highly targeted. Customers identified for 'High-Cost Intervention' can receive personalized offers, while those for 'Low-Cost Intervention' can get more standard discounts.\n",
        "*   **Resource Allocation**: The framework guides where to invest marketing and customer service resources, focusing on customers with high churn probability and a positive EV for intervention.\n",
        "*   **Continuous Improvement**: The business parameters (Cost of Churn, Cost of Intervention, Probability of Success) should be regularly reviewed and updated based on new data, A/B test results, and market conditions. This allows the model to adapt and improve its recommendations over time.\n",
        "*   **Feedback Loop**: Track the actual outcomes of interventions. Did customers who received a discount churn less? Was the predicted success rate accurate? Use this data to refine the `PROB_SUCCESS` parameters.\n",
        "\n",
        "By adopting this data-driven approach, the company can move from reactive customer retention to a proactive, financially optimized strategy, significantly impacting the bottom line.\n"
      ],
      "id": "0fd7d29d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ac66771"
      },
      "source": [
        "## Business Recommendations from Expected Value Analysis\n",
        "\n",
        "Based on the Expected Value (EV) framework and the analysis of various intervention strategies, here are the key findings and actionable business recommendations:\n",
        "\n",
        "### 1. Analysis of Total Expected Value\n",
        "\n",
        "Let's revisit the total expected values for each strategy:\n",
        "\n",
        "*   **No Intervention**: `$-452,433.74`\n",
        "    *   This represents the total potential loss if no retention efforts are made for any customer. It's a significant negative value, highlighting the high cost of churn.\n",
        "\n",
        "*   **Low-Cost Intervention**: `$-30,230.12`\n",
        "    *   Implementing a low-cost intervention (e.g., a 10% discount) for all customers with a churn probability above a certain threshold (if applied universally) results in a positive expected value, indicating a net financial gain compared to no intervention.\n",
        "\n",
        "*   **High-Cost Intervention**: `$-60,460.24`\n",
        "    *   Implementing a high-cost intervention (e.g., a 20% discount) for all customers with a churn probability above a certain threshold (if applied universally) yields an even higher positive expected value, suggesting that for certain customers, a more significant discount is more profitable.\n",
        "\n",
        "*   **Optimal Combined Strategy**: `$-87,502.83`\n",
        "    *   This is the most profitable strategy. By tailoring the intervention (No Intervention, Low-Cost, or High-Cost) to each customer based on their individual churn probability and the expected value of each action, the total expected value is maximized.\n",
        "\n",
        "### 2. Distribution of Optimal Strategies\n",
        "\n",
        "The `Distribution of Optimal Intervention Strategies` plot shows how many customers fall into each recommended category:\n",
        "\n",
        "*   A large portion of customers might still be recommended for 'No Intervention', either because their churn probability is low, or the cost of intervention outweighs the potential benefit.\n",
        "*   A significant number of customers are assigned to 'EV_Low_Intervention' and 'EV_High_Intervention', indicating that targeted discounts are financially beneficial for these segments.\n",
        "\n",
        "### 3. Business Recommendation: Implement an Adaptive, Optimal Combined Strategy\n",
        "\n",
        "**Recommendation**: The company should implement a dynamic, customer-specific retention strategy based on the 'Optimal Combined Strategy' identified by the Expected Value framework. This involves:\n",
        "\n",
        "1.  **Predicting Churn Probability**: For each customer, use the `selected_best_model` (Random Forest, in this case) to predict their churn probability.\n",
        "2.  **Calculating Expected Value for Interventions**: For each customer, calculate the Expected Value of 'No Intervention', 'Low-Cost Intervention', and 'High-Cost Intervention' using the defined business parameters.\n",
        "3.  **Selecting the Optimal Action**: For each customer, choose the intervention strategy that yields the highest Expected Value.\n",
        "\n",
        "### 4. Financial Implications\n",
        "\n",
        "Implementing the **Optimal Combined Strategy** is projected to generate an additional expected value of approximately **`$87,502.83`**. This is a substantial improvement compared to the `$-452,433.74` expected loss if **No Intervention** is undertaken, representing a potential swing of over **`$500,000`**.\n",
        "\n",
        "This framework ensures that resources (discounts) are allocated efficiently, targeting customers where the intervention is most likely to yield a positive financial return, and avoiding costly interventions for customers unlikely to churn or unlikely to be retained.\n",
        "\n",
        "### 5. Optimizing Customer Retention Efforts\n",
        "\n",
        "*   **Targeted Marketing**: Instead of generic marketing campaigns, retention efforts can be highly targeted. Customers identified for 'High-Cost Intervention' can receive personalized offers, while those for 'Low-Cost Intervention' can get more standard discounts.\n",
        "*   **Resource Allocation**: The framework guides where to invest marketing and customer service resources, focusing on customers with high churn probability and a positive EV for intervention.\n",
        "*   **Continuous Improvement**: The business parameters (Cost of Churn, Cost of Intervention, Probability of Success) should be regularly reviewed and updated based on new data, A/B test results, and market conditions. This allows the model to adapt and improve its recommendations over time.\n",
        "*   **Feedback Loop**: Track the actual outcomes of interventions. Did customers who received a discount churn less? Was the predicted success rate accurate? Use this data to refine the `PROB_SUCCESS` parameters.\n",
        "\n",
        "By adopting this data-driven approach, the company can move from reactive customer retention to a proactive, financially optimized strategy, significantly impacting the bottom line.\n"
      ],
      "id": "6ac66771"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaac77f1"
      },
      "source": [
        "## Business Recommendations from Expected Value Analysis\n",
        "\n",
        "Based on the Expected Value (EV) framework and the analysis of various intervention strategies, here are the key findings and actionable business recommendations:\n",
        "\n",
        "### 1. Analysis of Total Expected Value\n",
        "\n",
        "Let's revisit the total expected values for each strategy:\n",
        "\n",
        "*   **No Intervention**: `$-452,433.74`\n",
        "    *   This represents the total potential loss if no retention efforts are made for any customer. It's a significant negative value, highlighting the high cost of churn.\n",
        "\n",
        "*   **Low-Cost Intervention**: `$30,230.12`\n",
        "    *   Implementing a low-cost intervention (e.g., a 10% discount) for all customers with a churn probability above a certain threshold (if applied universally) results in a positive expected value, indicating a net financial gain compared to no intervention.\n",
        "\n",
        "*   **High-Cost Intervention**: `$60,460.24`\n",
        "    *   Implementing a high-cost intervention (e.g., a 20% discount) for all customers with a churn probability above a certain threshold (if applied universally) yields an even higher positive expected value, suggesting that for certain customers, a more significant discount is more profitable.\n",
        "\n",
        "*   **Optimal Combined Strategy**: `$87,502.83`\n",
        "    *   This is the most profitable strategy. By tailoring the intervention (No Intervention, Low-Cost, or High-Cost) to each customer based on their individual churn probability and the expected value of each action, the total expected value is maximized.\n",
        "\n",
        "### 2. Distribution of Optimal Strategies\n",
        "\n",
        "The `Distribution of Optimal Intervention Strategies` plot shows how many customers fall into each recommended category:\n",
        "\n",
        "*   A large portion of customers might still be recommended for 'No Intervention', either because their churn probability is low, or the cost of intervention outweighs the potential benefit.\n",
        "*   A significant number of customers are assigned to 'EV_Low_Intervention' and 'EV_High_Intervention', indicating that targeted discounts are financially beneficial for these segments.\n",
        "\n",
        "### 3. Business Recommendation: Implement an Adaptive, Optimal Combined Strategy\n",
        "\n",
        "**Recommendation**: The company should implement a dynamic, customer-specific retention strategy based on the 'Optimal Combined Strategy' identified by the Expected Value framework. This involves:\n",
        "\n",
        "1.  **Predicting Churn Probability**: For each customer, use the `selected_best_model` (Random Forest, in this case) to predict their churn probability.\n",
        "2.  **Calculating Expected Value for Interventions**: For each customer, calculate the Expected Value of 'No Intervention', 'Low-Cost Intervention', and 'High-Cost Intervention' using the defined business parameters.\n",
        "3.  **Selecting the Optimal Action**: For each customer, choose the intervention strategy that yields the highest Expected Value.\n",
        "\n",
        "### 4. Financial Implications\n",
        "\n",
        "Implementing the **Optimal Combined Strategy** is projected to generate an additional expected value of approximately **`$87,502.83`**. This is a substantial improvement compared to the `$-452,433.74` expected loss if **No Intervention** is undertaken, representing a potential swing of over **`$500,000`**.\n",
        "\n",
        "This framework ensures that resources (discounts) are allocated efficiently, targeting customers where the intervention is most likely to yield a positive financial return, and avoiding costly interventions for customers unlikely to churn or unlikely to be retained.\n",
        "\n",
        "### 5. Optimizing Customer Retention Efforts\n",
        "\n",
        "*   **Targeted Marketing**: Instead of generic marketing campaigns, retention efforts can be highly targeted. Customers identified for 'High-Cost Intervention' can receive personalized offers, while those for 'Low-Cost Intervention' can get more standard discounts.\n",
        "*   **Resource Allocation**: The framework guides where to invest marketing and customer service resources, focusing on customers with high churn probability and a positive EV for intervention.\n",
        "*   **Continuous Improvement**: The business parameters (Cost of Churn, Cost of Intervention, Probability of Success) should be regularly reviewed and updated based on new data, A/B test results, and market conditions. This allows the model to adapt and improve its recommendations over time.\n",
        "*   **Feedback Loop**: Track the actual outcomes of interventions. Did customers who received a discount churn less? Was the predicted success rate accurate? Use this data to refine the `PROB_SUCCESS` parameters.\n",
        "\n",
        "By adopting this data-driven approach, the company can move from reactive customer retention to a proactive, financially optimized strategy, significantly impacting the bottom line."
      ],
      "id": "eaac77f1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b63317f6"
      },
      "source": [
        "## Business Recommendations from Expected Value Analysis\n",
        "\n",
        "Based on the Expected Value (EV) framework and the analysis of various intervention strategies, here are the key findings and actionable business recommendations:\n",
        "\n",
        "### 1. Analysis of Total Expected Value\n",
        "\n",
        "Let's revisit the total expected values for each strategy:\n",
        "\n",
        "*   **No Intervention**: `$-452,433.74`\n",
        "    *   This represents the total potential loss if no retention efforts are made for any customer. It's a significant negative value, highlighting the high cost of churn.\n",
        "\n",
        "*   **Low-Cost Intervention**: `$30,230.12`\n",
        "    *   Implementing a low-cost intervention (e.g., a 10% discount) for all customers with a churn probability above a certain threshold (if applied universally) results in a positive expected value, indicating a net financial gain compared to no intervention.\n",
        "\n",
        "*   **High-Cost Intervention**: `$60,460.24`\n",
        "    *   Implementing a high-cost intervention (e.g., a 20% discount) for all customers with a churn probability above a certain threshold (if applied universally) yields an even higher positive expected value, suggesting that for certain customers, a more significant discount is more profitable.\n",
        "\n",
        "*   **Optimal Combined Strategy**: `$87,502.83`\n",
        "    *   This is the most profitable strategy. By tailoring the intervention (No Intervention, Low-Cost, or High-Cost) to each customer based on their individual churn probability and the expected value of each action, the total expected value is maximized.\n",
        "\n",
        "### 2. Distribution of Optimal Strategies\n",
        "\n",
        "The `Distribution of Optimal Intervention Strategies` plot shows how many customers fall into each recommended category:\n",
        "\n",
        "*   A large portion of customers might still be recommended for 'No Intervention', either because their churn probability is low, or the cost of intervention outweighs the potential benefit.\n",
        "*   A significant number of customers are assigned to 'EV_Low_Intervention' and 'EV_High_Intervention', indicating that targeted discounts are financially beneficial for these segments.\n",
        "\n",
        "### 3. Business Recommendation: Implement an Adaptive, Optimal Combined Strategy\n",
        "\n",
        "**Recommendation**: The company should implement a dynamic, customer-specific retention strategy based on the 'Optimal Combined Strategy' identified by the Expected Value framework. This involves:\n",
        "\n",
        "1.  **Predicting Churn Probability**: For each customer, use the `selected_best_model` (Random Forest, in this case) to predict their churn probability.\n",
        "2.  **Calculating Expected Value for Interventions**: For each customer, calculate the Expected Value of 'No Intervention', 'Low-Cost Intervention', and 'High-Cost Intervention' using the defined business parameters.\n",
        "3.  **Selecting the Optimal Action**: For each customer, choose the intervention strategy that yields the highest Expected Value.\n",
        "\n",
        "### 4. Financial Implications\n",
        "\n",
        "Implementing the **Optimal Combined Strategy** is projected to generate an additional expected value of approximately **`$87,502.83`**. This is a substantial improvement compared to the `$-452,433.74` expected loss if **No Intervention** is undertaken, representing a potential swing of over **`$500,000`**.\n",
        "\n",
        "This framework ensures that resources (discounts) are allocated efficiently, targeting customers where the intervention is most likely to yield a positive financial return, and avoiding costly interventions for customers unlikely to churn or unlikely to be retained.\n",
        "\n",
        "### 5. Optimizing Customer Retention Efforts\n",
        "\n",
        "*   **Targeted Marketing**: Instead of generic marketing campaigns, retention efforts can be highly targeted. Customers identified for 'High-Cost Intervention' can receive personalized offers, while those for 'Low-Cost Intervention' can get more standard discounts.\n",
        "*   **Resource Allocation**: The framework guides where to invest marketing and customer service resources, focusing on customers with high churn probability and a positive EV for intervention.\n",
        "*   **Continuous Improvement**: The business parameters (Cost of Churn, Cost of Intervention, Probability of Success) should be regularly reviewed and updated based on new data, A/B test results, and market conditions. This allows the model to adapt and improve its recommendations over time.\n",
        "*   **Feedback Loop**: Track the actual outcomes of interventions. Did customers who received a discount churn less? Was the predicted success rate accurate? Use this data to refine the `PROB_SUCCESS` parameters.\n",
        "\n",
        "By adopting this data-driven approach, the company can move from reactive customer retention to a proactive, financially optimized strategy, significantly impacting the bottom line.\n"
      ],
      "id": "b63317f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3b6962f"
      },
      "source": [
        "## Business Recommendations from Expected Value Analysis\n",
        "\n",
        "Based on the Expected Value (EV) framework and the analysis of various intervention strategies, here are the key findings and actionable business recommendations:\n",
        "\n",
        "### 1. Analysis of Total Expected Value\n",
        "\n",
        "Let's revisit the total expected values for each strategy:\n",
        "\n",
        "*   **No Intervention**: `$-452,433.74`\n",
        "    *   This represents the total potential loss if no retention efforts are made for any customer. It's a significant negative value, highlighting the high cost of churn.\n",
        "\n",
        "*   **Low-Cost Intervention**: `$30,230.12`\n",
        "    *   Implementing a low-cost intervention (e.g., a 10% discount) for all customers with a churn probability above a certain threshold (if applied universally) results in a positive expected value, indicating a net financial gain compared to no intervention.\n",
        "\n",
        "*   **High-Cost Intervention**: `$60,460.24`\n",
        "    *   Implementing a high-cost intervention (e.g., a 20% discount) for all customers with a churn probability above a certain threshold (if applied universally) yields an even higher positive expected value, suggesting that for certain customers, a more significant discount is more profitable.\n",
        "\n",
        "*   **Optimal Combined Strategy**: `$87,502.83`\n",
        "    *   This is the most profitable strategy. By tailoring the intervention (No Intervention, Low-Cost, or High-Cost) to each customer based on their individual churn probability and the expected value of each action, the total expected value is maximized.\n",
        "\n",
        "### 2. Distribution of Optimal Strategies\n",
        "\n",
        "The `Distribution of Optimal Intervention Strategies` plot shows how many customers fall into each recommended category:\n",
        "\n",
        "*   A large portion of customers might still be recommended for 'No Intervention', either because their churn probability is low, or the cost of intervention outweighs the potential benefit.\n",
        "*   A significant number of customers are assigned to 'EV_Low_Intervention' and 'EV_High_Intervention', indicating that targeted discounts are financially beneficial for these segments.\n",
        "\n",
        "### 3. Business Recommendation: Implement an Adaptive, Optimal Combined Strategy\n",
        "\n",
        "**Recommendation**: The company should implement a dynamic, customer-specific retention strategy based on the 'Optimal Combined Strategy' identified by the Expected Value framework. This involves:\n",
        "\n",
        "1.  **Predicting Churn Probability**: For each customer, use the `selected_best_model` (Random Forest, in this case) to predict their churn probability.\n",
        "2.  **Calculating Expected Value for Interventions**: For each customer, calculate the Expected Value of 'No Intervention', 'Low-Cost Intervention', and 'High-Cost Intervention' using the defined business parameters.\n",
        "3.  **Selecting the Optimal Action**: For each customer, choose the intervention strategy that yields the highest Expected Value.\n",
        "\n",
        "### 4. Financial Implications\n",
        "\n",
        "Implementing the **Optimal Combined Strategy** is projected to generate an additional expected value of approximately **`$87,502.83`**. This is a substantial improvement compared to the `$-452,433.74` expected loss if **No Intervention** is undertaken, representing a potential swing of over **`$500,000`**.\n",
        "\n",
        "This framework ensures that resources (discounts) are allocated efficiently, targeting customers where the intervention is most likely to yield a positive financial return, and avoiding costly interventions for customers unlikely to churn or unlikely to be retained.\n",
        "\n",
        "### 5. Optimizing Customer Retention Efforts\n",
        "\n",
        "*   **Targeted Marketing**: Instead of generic marketing campaigns, retention efforts can be highly targeted. Customers identified for 'High-Cost Intervention' can receive personalized offers, while those for 'Low-Cost Intervention' can get more standard discounts.\n",
        "*   **Resource Allocation**: The framework guides where to invest marketing and customer service resources, focusing on customers with high churn probability and a positive EV for intervention.\n",
        "*   **Continuous Improvement**: The business parameters (Cost of Churn, Cost of Intervention, Probability of Success) should be regularly reviewed and updated based on new data, A/B test results, and market conditions. This allows the model to adapt and improve its recommendations over time.\n",
        "*   **Feedback Loop**: Track the actual outcomes of interventions. Did customers who received a discount churn less? Was the predicted success rate accurate? Use this data to refine the `PROB_SUCCESS` parameters.\n",
        "\n",
        "By adopting this data-driven approach, the company can move from reactive customer retention to a proactive, financially optimized strategy, significantly impacting the bottom line.\n"
      ],
      "id": "e3b6962f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef9e9e83"
      },
      "source": [
        "## Business Recommendations from Expected Value Analysis\n",
        "\n",
        "Based on the Expected Value (EV) framework and the analysis of various intervention strategies, here are the key findings and actionable business recommendations:\n",
        "\n",
        "### 1. Analysis of Total Expected Value\n",
        "\n",
        "Let's revisit the total expected values for each strategy:\n",
        "\n",
        "*   **No Intervention**: `$-452,433.74`\n",
        "    *   This represents the total potential loss if no retention efforts are made for any customer. It's a significant negative value, highlighting the high cost of churn.\n",
        "\n",
        "*   **Low-Cost Intervention**: `$30,230.12`\n",
        "    *   Implementing a low-cost intervention (e.g., a 10% discount) for all customers with a churn probability above a certain threshold (if applied universally) results in a positive expected value, indicating a net financial gain compared to no intervention.\n",
        "\n",
        "*   **High-Cost Intervention**: `$60,460.24`\n",
        "    *   Implementing a high-cost intervention (e.g., a 20% discount) for all customers with a churn probability above a certain threshold (if applied universally) yields an even higher positive expected value, suggesting that for certain customers, a more significant discount is more profitable.\n",
        "\n",
        "*   **Optimal Combined Strategy**: `$87,502.83`\n",
        "    *   This is the most profitable strategy. By tailoring the intervention (No Intervention, Low-Cost, or High-Cost) to each customer based on their individual churn probability and the expected value of each action, the total expected value is maximized.\n",
        "\n",
        "### 2. Distribution of Optimal Strategies\n",
        "\n",
        "The `Distribution of Optimal Intervention Strategies` plot shows how many customers fall into each recommended category:\n",
        "\n",
        "*   A large portion of customers might still be recommended for 'No Intervention', either because their churn probability is low, or the cost of intervention outweighs the potential benefit.\n",
        "*   A significant number of customers are assigned to 'EV_Low_Intervention' and 'EV_High_Intervention', indicating that targeted discounts are financially beneficial for these segments.\n",
        "\n",
        "### 3. Business Recommendation: Implement an Adaptive, Optimal Combined Strategy\n",
        "\n",
        "**Recommendation**: The company should implement a dynamic, customer-specific retention strategy based on the 'Optimal Combined Strategy' identified by the Expected Value framework. This involves:\n",
        "\n",
        "1.  **Predicting Churn Probability**: For each customer, use the `selected_best_model` (Random Forest, in this case) to predict their churn probability.\n",
        "2.  **Calculating Expected Value for Interventions**: For each customer, calculate the Expected Value of 'No Intervention', 'Low-Cost Intervention', and 'High-Cost Intervention' using the defined business parameters.\n",
        "3.  **Selecting the Optimal Action**: For each customer, choose the intervention strategy that yields the highest Expected Value.\n",
        "\n",
        "### 4. Financial Implications\n",
        "\n",
        "Implementing the **Optimal Combined Strategy** is projected to generate an additional expected value of approximately **`$87,502.83`**. This is a substantial improvement compared to the `$-452,433.74` expected loss if **No Intervention** is undertaken, representing a potential swing of over **`$500,000`**.\n",
        "\n",
        "This framework ensures that resources (discounts) are allocated efficiently, targeting customers where the intervention is most likely to yield a positive financial return, and avoiding costly interventions for customers unlikely to churn or unlikely to be retained.\n",
        "\n",
        "### 5. Optimizing Customer Retention Efforts\n",
        "\n",
        "*   **Targeted Marketing**: Instead of generic marketing campaigns, retention efforts can be highly targeted. Customers identified for 'High-Cost Intervention' can receive personalized offers, while those for 'Low-Cost Intervention' can get more standard discounts.\n",
        "*   **Resource Allocation**: The framework guides where to invest marketing and customer service resources, focusing on customers with high churn probability and a positive EV for intervention.\n",
        "*   **Continuous Improvement**: The business parameters (Cost of Churn, Cost of Intervention, Probability of Success) should be regularly reviewed and updated based on new data, A/B test results, and market conditions. This allows the model to adapt and improve its recommendations over time.\n",
        "*   **Feedback Loop**: Track the actual outcomes of interventions. Did customers who received a discount churn less? Was the predicted success rate accurate? Use this data to refine the `PROB_SUCCESS` parameters.\n",
        "\n",
        "By adopting this data-driven approach, the company can move from reactive customer retention to a proactive, financially optimized strategy, significantly impacting the bottom line.\n"
      ],
      "id": "ef9e9e83"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dbeb803"
      },
      "source": [
        "## Business Recommendations from Expected Value Analysis\n",
        "\n",
        "Based on the Expected Value (EV) framework and the analysis of various intervention strategies, here are the key findings and actionable business recommendations:\n",
        "\n",
        "### 1. Analysis of Total Expected Value\n",
        "\n",
        "Let's revisit the total expected values for each strategy:\n",
        "\n",
        "*   **No Intervention**: `$-452,433.74`\n",
        "    *   This represents the total potential loss if no retention efforts are made for any customer. It's a significant negative value, highlighting the high cost of churn.\n",
        "\n",
        "*   **Low-Cost Intervention**: `$30,230.12`\n",
        "    *   Implementing a low-cost intervention (e.g., a 10% discount) for all customers with a churn probability above a certain threshold (if applied universally) results in a positive expected value, indicating a net financial gain compared to no intervention.\n",
        "\n",
        "*   **High-Cost Intervention**: `$60,460.24`\n",
        "    *   Implementing a high-cost intervention (e.g., a 20% discount) for all customers with a churn probability above a certain threshold (if applied universally) yields an even higher positive expected value, suggesting that for certain customers, a more significant discount is more profitable.\n",
        "\n",
        "*   **Optimal Combined Strategy**: `$87,502.83`\n",
        "    *   This is the most profitable strategy. By tailoring the intervention (No Intervention, Low-Cost, or High-Cost) to each customer based on their individual churn probability and the expected value of each action, the total expected value is maximized.\n",
        "\n",
        "### 2. Distribution of Optimal Strategies\n",
        "\n",
        "The `Distribution of Optimal Intervention Strategies` plot shows how many customers fall into each recommended category:\n",
        "\n",
        "*   A large portion of customers might still be recommended for 'No Intervention', either because their churn probability is low, or the cost of intervention outweighs the potential benefit.\n",
        "*   A significant number of customers are assigned to 'EV_Low_Intervention' and 'EV_High_Intervention', indicating that targeted discounts are financially beneficial for these segments.\n",
        "\n",
        "### 3. Business Recommendation: Implement an Adaptive, Optimal Combined Strategy\n",
        "\n",
        "**Recommendation**: The company should implement a dynamic, customer-specific retention strategy based on the 'Optimal Combined Strategy' identified by the Expected Value framework. This involves:\n",
        "\n",
        "1.  **Predicting Churn Probability**: For each customer, use the `selected_best_model` (Random Forest, in this case) to predict their churn probability.\n",
        "2.  **Calculating Expected Value for Interventions**: For each customer, calculate the Expected Value of 'No Intervention', 'Low-Cost Intervention', and 'High-Cost Intervention' using the defined business parameters.\n",
        "3.  **Selecting the Optimal Action**: For each customer, choose the intervention strategy that yields the highest Expected Value.\n",
        "\n",
        "### 4. Financial Implications\n",
        "\n",
        "Implementing the **Optimal Combined Strategy** is projected to generate an additional expected value of approximately **`$87,502.83`**. This is a substantial improvement compared to the `$-452,433.74` expected loss if **No Intervention** is undertaken, representing a potential swing of over **`$500,000`**.\n",
        "\n",
        "This framework ensures that resources (discounts) are allocated efficiently, targeting customers where the intervention is most likely to yield a positive financial return, and avoiding costly interventions for customers unlikely to churn or unlikely to be retained.\n",
        "\n",
        "### 5. Optimizing Customer Retention Efforts\n",
        "\n",
        "*   **Targeted Marketing**: Instead of generic marketing campaigns, retention efforts can be highly targeted. Customers identified for 'High-Cost Intervention' can receive personalized offers, while those for 'Low-Cost Intervention' can get more standard discounts.\n",
        "*   **Resource Allocation**: The framework guides where to invest marketing and customer service resources, focusing on customers with high churn probability and a positive EV for intervention.\n",
        "*   **Continuous Improvement**: The business parameters (Cost of Churn, Cost of Intervention, Probability of Success) should be regularly reviewed and updated based on new data, A/B test results, and market conditions. This allows the model to adapt and improve its recommendations over time.\n",
        "*   **Feedback Loop**: Track the actual outcomes of interventions. Did customers who received a discount churn less? Was the predicted success rate accurate? Use this data to refine the `PROB_SUCCESS` parameters.\n",
        "\n",
        "By adopting this data-driven approach, the company can move from reactive customer retention to a proactive, financially optimized strategy, significantly impacting the bottom line.\n"
      ],
      "id": "9dbeb803"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b26e380b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The most profitable discount strategy recommended is the \"Optimal Combined Strategy,\" which is projected to generate a total expected value of \\$87,502.83. This strategy involves tailoring interventions (No Intervention, Low-Cost, or High-Cost) to each customer based on their individual churn probability and the expected value of each action, rather than applying a single strategy across the board.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   Churn probabilities were successfully calculated for the test set using the `selected_best_model`.\n",
        "*   Illustrative business parameters were defined for the Expected Value (EV) framework: Cost of Churn (\\$500), Cost of High-Value Intervention (\\$100), Cost of Low-Value Intervention (\\$50), Probability of Success for High Intervention (0.60), and Probability of Success for Low Intervention (0.30).\n",
        "*   The total Expected Value for each strategy across all customers was calculated as:\n",
        "    *   No Intervention: $-\\$452,433.74$\n",
        "    *   Low-Cost Intervention: $\\$30,230.12$\n",
        "    *   High-Cost Intervention: $\\$60,460.24$\n",
        "    *   Optimal Combined Strategy: $\\$87,502.83$\n",
        "*   The \"Optimal Combined Strategy\" yields the highest total expected value, demonstrating a significant financial benefit of \\$87,502.83 by personalizing retention efforts.\n",
        "*   Implementing the \"Optimal Combined Strategy\" represents a potential financial swing of over \\$500,000 compared to taking no retention action (from a loss of \\$452,433.74 to a gain of \\$87,502.83).\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Implement a dynamic, customer-specific retention strategy by predicting churn probability for each customer, calculating the Expected Value of various interventions, and selecting the optimal action that yields the highest EV.\n",
        "*   Continuously review and update the business parameters (Cost of Churn, Cost of Intervention, Probability of Success) based on new data, A/B testing, and market conditions to refine and improve the model's recommendations over time.\n"
      ],
      "id": "b26e380b"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 60.005711,
      "end_time": "2021-06-29T09:03:36.232671",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-06-29T09:02:36.226960",
      "version": "2.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}